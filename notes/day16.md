## [DAY 16] NLP - 1차
### (1강) Intro to NLP, Bag-of-Words
#### 1. Natural Language Processiong
+ 자연어 처리는 컴퓨터가 주어진 단어나 문장 보다 긴 문단이나 길을 이해하는 NLU라는 과정이 있고, 자연어를 적절히 생성할 수 있는 NLG라는 테스크가 존재함
+ 자연어는 이미지 분야와 함께 활발하게 발전하고 있는 분야
+ Low-level parsing
    + Tokenization : 문장을 단어 단위로 쪼개는 것
    + Stemming : 수많은 어미의 변화 속에서도 같은 의미를 나타낸다는 것을 컴퓨터가 알 수 있게하는 것(=어근에 대한 이해)
+ Word and phrase level
    + Named entity recognition(NER) : Newyork times는 개별단어로 해석하지 말고 하나로 해석해야함
    + Part-of-speech(POS) : 단어들이 문장 내에서 품사나 성분이 무엇인지 파악하는 테스크 (명사, 부사구, 형용사구 등)
+ Setence level
    + Sentment analysis : 문장이 부정, 긍정인지 감정 어조를 파악하는 방법
    + machine translation : 번역하는 경우, 영어 문장을 이해하고 각 단어 별로 적절한 한글 단어와 어순을 맞춰서 번역해야함
+ Multi-sentence and paragraph level
    + Entailment prediction : 두 문장간의 논리적인 내포, 모순 관계를 예측함
    + question answering : 질문을 이해하고 정확하게 답변을 함
    + dialog systems : 챗봇과 같은 대화를 수행할 수 있는 대화처리 기술
    + summarization : 뉴스를 한 줄 요약의 형태로 나타낼 수 있는 테스크

#### 2. Text mining
+ 글이나 문서에서 정보와 인사이트를 추출
+ Topic Modeling : 서로 다른 키워드지만 비슷한 의미를 가지는 키워드를 그룹핑할 수 있는 기술
+ 사회과학적인 인사이트를 발견하는데 활용

#### 3. Information retrieval
+ 검색 기술을 연구하는 분야 : 검색 성능이 고도화되면서 이 기술은 성숙됨
+ 추천 시스템을 포함하는 기술, 음악, 영상 등에 대한 추천 서비스 등 다양한 분야에서 활용

#### 4. 자연어 처리 발전
+ 자연어 처리 분야는 느리지만 꾸준히 발전해오고 있음, ML,DL 기술을 숫자로 이뤄진 데이터가 필요함
+ 텍스트 데이터를 단어 단위로 분류하고 벡터로 표현하는 과정을 거침 ( word embedding )
+ 순서 정보를 포함하는 시퀀스 데이터를 처리하는데 특화된 RNN이라는 모델이 주로 사용됨
+ RNN 기반의 자연어 모델에서 최근에는 Transformer 모델 구조를 기반으로 함
+ 과거에는 언어 간의 문장구조, 어순 등 룰 베이스로 번역했었음
+ Transformer은 자연어 처리뿐만 아니라 이미지, 신약 개발 등 다양한 분야에서 활용
+ self-attention 모듈을 계속 쌓아나가는 식으로 모델의 크기를 키우고, 자가 지도학습이라는 범용적 태스크를 사용하여 모델을 학습
+ 자가 지도학습(self-supervised)은 입력 중 일부 단어를 가리고 앞뒤 문맥을 보고 해당 단어를 맞추는 학습을 진행함 (BERT, GPT-3 등)

#### 5. Bag-of-words
+ 단어 및 문서를 숫자 형태로 나타내는 기법
+ 이를 활용한 문서 분류 기법 NaiveBayes Classifier
+ 단계
    + STEP1 : 문장에서 중복된 단어를 제거하고 사전에 단어들을 등록
    + STEP2 : 각각의 단어를 범주형 변수로 one-hot vector로 표현함, 각 단어 별로 특정 차원 값이 1인 벡터들을 생성함
    + STEP3 : 문장에 포함된 단어들의 one-hot 벡터를 모두 더해서 표현할 수 있고 이를 Bag-of-Words 벡터라고 얘기함

#### 6. NaiveBayes Classifier
+ 문서가 분류될 수 있는 클래스가 C개가 존재함(정치, 경제, 스포츠, 연애 -> 
4개)
+ 특정 문서 d가 주어졌을 때 C에 속할 확률 분포는 P(c|d)로 표현이 가능
+ P(d)는 문서가 뽑힐 확률을 의미하는데 상수로 표현가능 함(이 값은 argmax에서 무시할 수 있기 때문에 최종적으로 마지막 수식이 반환)

![캡처](https://user-images.githubusercontent.com/44515744/107897045-759fce80-6f7b-11eb-8253-eadd5daf9925.PNG)

+ 각각의 클래스가 갖는 확률값을 구한 뒤, 클래스 안에 각 단어가 나타날 확률들을 모두 곱함 (단어 출현 수 / 총 단어 개수)
+ 새로운 문장이 나올 경우 아래와 같이 클래스 확률 * 단어의 확률을 모든 곱한 최종 확률 값이 가장 큰 것으로 분류한다.
+ 학습 데이터 내에 특정 단어가 발견되지 않았을 경우, 이 확률은 0으로 나타날 것이므로, 다른 단어들이 밀접한 관련이 있더라도 해당 클래스로 분류가 불가능해짐 (정규화 방식이 추가되어 함께 사용됨)

![캡처](https://user-images.githubusercontent.com/44515744/107897759-52761e80-6f7d-11eb-84b0-b6e1eeca9e0b.PNG)

### (2강) Word Embedding
#### 1. Word Embedding
+ 단어의 3차원 공간상의 벡터로 표현하는 방법
+ 유사한 의미를 갖는 단어 사이의 거리를 짧게 표현함
+ 서로 다른 의미를 갖는 단어는 거리를 멀게한다.
+ 텍스트 데이터를 학습데이터로 주고 디멘션 수를 사전에 정의해서 

#### 2. Word2Vec
+ 같은 문장에서 나타난 인접한 단어들 간의 의미가 비슷할 것이라는 가정을 사용하는 알고리즘
+ 특정 단어에 대한 주변 단어들의 확률분포를 예측하게 됨
+ STEP1 : 문장에서 워드 별로 분류하는 Tokenization을 진행
+ STEP2 : 사전을 구축하게 됨
+ STEP3 : 사전의 단어들은 One-hot 벡터의 형태를 갖게 됨
+ STEP4 : 슬라이딩 윈도우 기법을 이용해서 한 단어를 중심으로 앞뒤로 나타난 Word 각각과 입출력 단어 쌓을 구성
    + CBOW : Continuous Bag Of Word ( 주변 단어들을 보고 중심 단어를 맞추는 것)
    + Skip-gram : 중심 단어를 보고 주변 단어들을 맞추는 것

#### 3. Word intrusion detection
+ 여러 단어들이 주어져있을 때 나머지 단어와 의미가 가장 상이한 단어를 찾는 것, 단어별로 계산된 유클리드 디스턴스의 평균 값이 가장 큰 것이 의미한 상이한 단어가 되는 것

#### 4. GloVe
+ 입출력 단어 쌍들에 대해서 두 단어가 한 윈도우 내에서 총 몇번 동시에 등장했는지 사전에 계산을 하고, 이것에 대한 확률을 구해서 로스 함수로 활용함
+ Word2Vec가 특정한 입출력 단어쌍이 자주 등장할 경우, 이러한 아이템이 여러번에 걸쳐 학습됨으로써 내적값이 비례해서 커지게되는 학습 방식이었다면,
+ Glove는 단어쌍이 동시에 등장한 횟수를 미리 계산하고 활용하여 중복 연산을 제거해서 속도가 빠름

![캡처](https://user-images.githubusercontent.com/44515744/107904759-0d0f1c80-6f90-11eb-996a-3921435e2cef.PNG)

### 추가 학습 내용
#### 1. StarGan
+ 이미지 변환 : 어떤 이미지를 특정한 특징을 가진 이미지로 변환하는 것
+ 도메인 : 변환할 때 적용하는 특징
    + 도메인을 2개로 구분된 이유는 학습한 데이터 셋이 다르기 때문, 단일 모델에서 2개의 데이터 셋을 학습하여 출력한 결과

+ 문제
    + 최근 연구에서 사용되는 도메인-변환 기법은 두 개 이상의 도메인을 학습할 때 확장성(Scalability)과 견고함이 부족(Robustness,강건성) 부족
    + 기존의 연구에서는 대부분 하나의 특징만을 학습해서 변환하는 방법들을 제시했었음
    + 기존에 여러 도메인을 학습하기 위한 교차 도메인 모델이 존재했는데 도메인마다 여러개의 신경망을 사용했어야 해서 불필요한 연산이 많았다. (+ 특정 도메인만 특화해서 학습하기 때문에 일반적인 정보를 얻지 못함)

+ 기여
    + StarGAN은 하나의 통합된 모델에서 다중-도메인과 다양한 데이터 셋을 하나의 네트워크에서 학습할 수 있게 해줌
    

#### 2. StarGan의 단일 모델
+ StarGan은 하나의 신경망을 이용해서 많은 도메인으로 변환하기 때문에 하나의 도메인에만 특화되지 않고 일반적인 지식을 학습하여 더 높은 퀄리티의 이미지를 생성함 ( 만약에 우는 표정에 대한 변환을 하고싶으면, 다른 표정의 이미지들도 데이터셋으로 활용하면 이미지 표현에 있어서 더 자연스럽고 높은 퀄리티의 이미지를 생성하는 것으로 이해)
+ StarGan의 아키텍처
    + GAN에서는 잠재 변수 z를 입력값으로 받는 반면, StarGAN은 도메인 정보(c)와 원본 이미지(x)를 입력값으로 받는다.

+ Loss Function
+ 기존의 적대적 생성망의 손실 함수를 사용하는 데 몇 가지 계산을 추가했다고 함
+ 기존의 Discriminator 의 손실 함수인 L_D, Generator 손실 함수인 L_G를 아래 네가지 함수를 조합해서 생성함
    + Adversarial Loss : GAN의 MinMax-Game과 동일하게 정의하는데 입력값으로 원본 이미지를 그대로 사용함
    + Domain Classification at Discriminator : Discriminator는 이미지의 원본,가짜 구분뿐만 아니라 원본 이미지의 도메인까지 예측해야함
    + Domain Classification at Generator : Generator 또한 도메인을 학습해야한다.
    + Image Reconstruction Loss : 변환되는 과정에서의 이미지 퀄리티를 지키기 위해서 사용

#### 3. 다중 데이터 셋을 통합하여 학습
+ Mask Vector : 두 개의 데이터 셋의 Class를 통합하여 학습하기 위해 두 가지 클래스를 모두 만족하는 One-hot vector를 만드는데 이것은 Mask Vector라고 함
+ 데이터 셋에 포함된 이미지 개수가 다를 경우, 특정 데이터 셋에 편향되어 학습될 수 있으므로, 개수가 적은 데이터셋을 학습할 때 epoch를 더 많이줘서 균등하게 학습하게 해준다고 함
