## [DAY 18] NLP - 3차
### (5강) Sequence to Sequence with Attention
#### 1. Seq2Seq Model
+ RNN의 many to many 형태에 해당함
+ 입력 시퀀스를 모두 읽은 후 출력 시퀀스를 생성,예측하는 모델
+ 입력은 워드 단위의 문장, 출력도 예측해야하는 문장이 됨
+ 입력 문장을 읽어내는 RNN 모델을 인코더, 출력 문장을 생성하는 RNN 모델을 디코더라고 함
+ 서로 파라미터를 Share하지 않고 디코더 모델을 구성함
+ LSTM을 모델로 채용함
+ 입력의 마지막의 히든 스테이트 벡터는 첫번째 출력의 히든 스테이트 벡터로 활용함
+ <SOS> 등의 특수어들을 Vocab에 정의하고 Decoder 최초의 타임 스텝에 넣어줌
+ Decoder 마지막에는 <EOS> 토큰이 나올때까지 디코더 RNN을 구동함

![캡처](https://user-images.githubusercontent.com/44515744/108143314-d27dbf00-710a-11eb-9826-d1bab485fcae.PNG)

#### 2. Seq2Seq Model with Attention
+ 히든 스테이트 벡터의 디멘션이 정해져있어서, 짧은 문장이든 긴 문장이든 마지막 타임스텝에 있는 히든 스테이트 벡터에 많은 정보들을 우겨넣어야함 
+ 마지막 타임스텝을 기준으로 할때 훨씬 이전에 대한 정보는 변질되거나 소실될 가능성이 많음
+ 앞쪽에 나타난 정보들을 잘 저장하지 못하는 현상이 발생
+ 차선책으로써 이전의 트릭은 입력 문장의 순서를 뒤집어서 사용했었음 ( I Go Home -> Home Go I)
    + 문장의 초반에 대한 정보를 잘 살려서 디코더로 전달
+ Attention 모듈을 사용할 때의 Problem Setting
+ 디코더에서는 인코더의 마지막에서 나온 히든 스테이트 벡터에만 의존한 것이 아니라, 각각의 단어들을 순차적으로 인코딩하면서 나온 인코더 히든 스테이트 벡터들을 전체적으로 디코더에 제공
+ 디코더에서는 각 타임스텝에서 단어를 생성할때 필요한 인코더 스테이트 벡터를 선별적으로 가져가서 예측에 활용

#### 3. Attention 모듈의 작동원리
+ Encoder의 히든 스테이트 벡터들이 디코더의 첫번째 타임스텝의 히든 스테이트 벡터의 입력값으로 사용
+ 디코더의 히든 스테이트 벡터를 갖고 인코더에서 주어진 4개의 히든 스테이트 벡터 중 어떤 것이 필요한지 선별함
+ 디코더 스테이트 벡터가 인코더 히든 스테이트 벡터 4개와 내적연산을 수행함 -> Attention Scores
    + 내적 값은 디코더 히든 스테이트 벡터와 인코더 스테이트 벡터와의 유사도를 계산
+ Attension Scores를 소프트맥스 함수를 통해서 변환하고 결과를 가중치로 활용할 수 있음
    + 이 가중치를 각 인코더의 히든 스테이트 벡터와 연산함, 이렇게 만들어진 가중 평균된 벡터를 Attention Ouput 벡터 또는 컨텍스트 벡터라고도 함
+ 최종적으로 Decoder 히든 스테이트 벡터와 컨텍스트 벡터가 concat이 되어 아웃풋 layer의 입력으로 들어가 결과 값을 예측
+ 합이 1인 가중치를 Attention Vector라고 함
+ Decoder RNN은 아웃풋 데이터를 예측, 필요로 하는 정보를 (취사)선택하는데 활용

![캡처](https://user-images.githubusercontent.com/44515744/108144162-7320ae80-710c-11eb-8dc9-063b9e46bcf4.PNG)

+ 역전파시에 decoder는 예측을 잘하고 원하는 가중치를 선택할 수 있도록, 학습됨

![캡처](https://user-images.githubusercontent.com/44515744/108144511-32756500-710d-11eb-91c8-b7c090cdbdf8.PNG)

+ 학습시에는 예측값이 다음 학습의 입력값으로 쓰이지 않음(학습 초반에 잘못된 학습을 방지)
+ 올바른 입력값을 매타임스텝마다 입력(Teacher Forcing)
    + 학습이 용이하지만, 테스트와 실제 사용시 결과에 괴리가 존재할 수 있음
+ 학습 후반부에는 Teacher Forcing이 아닌 예측값을 입력하는 방식으로 학습을 변화시키는 방식도 사용됨

#### 4. 어텐션 스코어를 구하는 방법
+ 일반적인 내적을 사용 (dot)
+ 가중치 행렬의 사용 (general)
    + 가중치 행렬은 서로다른 디멘션을 가지는 벡터들 간의 곱해진 값들에 각각 부여되는 가중치로 사용됨

![캡처](https://user-images.githubusercontent.com/44515744/108146026-c3e5d680-710f-11eb-9589-b25a1a730985.PNG)

+ 콘캣을 사용 (concat)
    + 입력으로 주어지는 decoder hidden state 벡터와 인코더의 벡터가 주어졌을 때 스칼라 값이 나올 수 있게되는 유사도가 나올 수 있는 추가적인 학습가능한 뉴럴 네트워크를 추가할 수 있음
    + 콘캣 후, 선형 변환을 거치고 활성화함수를 적용하고 두번째 선형 변환을 거치게 함

#### 5. 어텐션의 장점과 특성
+ 기존의 Seq2Seq에 어텐션이 추가되면서 기계 번역분야에서의 성능을 올려줌
    + 디코더의 매 타임스텝마다 입력 시퀀스의 어떤 부분을 활용해서 예측할지를 선택할 수 있음
+ 긴 문장에 대한 번역이 어려운 것을 해결
+ 학습 관점에서 기울기 소실이 발생되던 것을 어텐션을 사용해서 지름길을 통해서 전달하여 학습이 가능
+ 디코더가 예측할때 인코더 상의 어떤 단어에 집중했는지를 파악할 수 있음
    + 어텐션은 단어의 순서뿐만 아니라 단어가 입력될 때 여러 단어들을 참고할 수 있음

### (6강) Beam Search and BLEU
#### 1. Beam Search란?
+ Seq2Seq 자연어 생성 모델의 테스트 타임에서 보다 좋은 품질의 생성결과를 얻을 수 있게하는 기법
+ Greedy decoding : 매 타임스텝마다 가장 높은 확률을 갖는 단어를 택해서 디코딩을 진행
    + 예측한 결과가 중간에 잘못되면 되돌릴 수 없어 잘못된 결과를 반환하게 됨

#### 2. Exhaustive Search란?
+ 출력 문장 y, 첫번째 출력 y_1, 입력값을 x라 했을때
+ x가 주어졌을 때 각 단어의 순차적인 출력에 대한 확률이 최대값이 되는 것을 원함
+ 타임 스탭 t까지의 가능한 모든 Vocab들을 V^t만큼 경우의 수를 고려해야함 ( 과도한 연산 발생 )

![캡처](https://user-images.githubusercontent.com/44515744/108148548-62743680-7114-11eb-89b8-4037cf96d2b7.PNG)

#### 3. Beam search
+ Greedy와 Exhaustive Search의 중간에 있는 차선책의 Approach
+ Decoder의 매 타임스텝 마다 정해놓은 k개의 가지수를 정해놓고 k개(beam size)의 아웃풋을 가설(hypothesis)이라고 함

![캡처](https://user-images.githubusercontent.com/44515744/108148828-f6460280-7114-11eb-8bc9-afd00e7d46e1.PNG)

+ 타임 스텝마다 가장 높은 k의 후보를 고려하고 추적하는 것이 Beam Search의 핵심 아이디어
+ 모든 경우의 수를 다 따지지는 않음
+ 예측 값은 Vocab 상에서 확률분포로 아웃풋이 나타남, Greedy Decoding이면 가장 높은 확률 값을 뽑겠지만,
+ Beam size가 2이면 가장 확률 값이 높은 두 단어를 뽑게됨
+ 확률 값은 0~1 사이의 값을 나타냄, Log에서는 0~1 사이의 값은 -이지만 log는 단조증가 함으로 더 높은 확률을 표현할 수 있음
+ 두 단어 이후에 나오게될 가장 높은 확률을 가진 두 단어(k^2개)를 추가로 뽑음
+ 이를 통해서 가장 확률이 높은 k개의 hypotheses를 선택하게 돰

![캡처](https://user-images.githubusercontent.com/44515744/108149469-407bb380-7116-11eb-9411-2636f8a751f6.PNG)

#### 4. Stopping Criterion
+ Greedy decoding : <END> 토큰이 나오기 전까지 진행
+ Beam Search Decoding
    + 각 hypotheses마다 <END>가 나오는 타임 스텝이 다를 수 있음
    + 임시 저장공간에 저장된 <END> 토큰이 n개가 되면 종료하거나, T만큼의 타임스텝만큼 중단할 수 있음

#### 5. Finishing up
+ 가장 높은 스코어를 갖는 hypotheses를 예측값으로 사용
+ 시퀀스의 길이가 다를 경우에는 짧은 길이를 가진 것이 joint probablity값이 높음
    + hypotheses별로 타임스텝만큼 나눈 결과를 비교

![캡처](https://user-images.githubusercontent.com/44515744/108150071-65245b00-7117-11eb-8899-633af0c07b9e.PNG)

#### 6. 자연어 생성 모델 품질 평가 방법
+ 자연어 생성 모델에서 품질과 결과 정확도를 평가하는 척도
+ 고정된 위치에서만 평가하는 방법은 추론 값이 I Love You인데 Oh I Love You를 예측했다면 정확도가 0이 됨
+ precision(정밀도)는 (겹치는 단어의 수) / (예측한 문장의 길이)로 계산
    + ex) 검색된 문서들 중 관련 있는 문서들의 비율
    + TP / (TP + FP)
+ recall(재현율)은 (겹치는 단어의 수) / (추론한 문장의 길이)로 계산 
    + ex) 관련 있는 문서들 중 실제로 검색된 문서들의 비율
    + ex) 스타크래프트에서 소환해야할 대상 중에서 얼만큼을 소환했는가?
    + TP / (TP + FN)

+ F - measure은 위 두 값을 대표하기 위한 조화 평균을 활용 (작은 값에 좀더 치중하려는 의도)
    + 산술 평균 : (A + B)/2
    + 기하 평균 : (A x B)^{1/2}
    + 조화 평균 : 1 / ((1/A + 1/B)/2)
    + 산술 평균 >= 기하 평균 >= 조화 평균      - 우측으로 갈수록 작은 값에 더 가중치를 주는 방식임

![캡처](https://user-images.githubusercontent.com/44515744/108151306-c0574d00-7119-11eb-8cfd-4466ac61b14c.PNG)

+ 이 방식을 사용하면 단어의 순서가 달라져도 정확도에 패널티가 없음

![캡처](https://user-images.githubusercontent.com/44515744/108151512-365bb400-711a-11eb-8cea-e7ac3b479ff6.PNG)

#### 7. BiLingual Evaluation Understudy (BLEU)란?
+ 개별 단어레벨에서 봤을 때, 얼마나 공통적으로 Ground Truth와 겹치는지에 대한것뿐만 아니라, 연속된 두 개의 단어 혹은 N개의 단어의 문구가 Ground Truth와 얼마나 겹치는지 파악
+ 계산 수식에서 recall은 무시함
+ Precision은 주어진 문장과 예측 문장이 얼마나 겹치는지에 대한 내용임
+ 1-gram은 한 단어가 실제 문장과 얼마나 일치하는 지에 대한 정확도
+ 2-gram(bi-gram)은 두 개의 연속된 단어가 얼마나 문장에서 일치하는 지에 대한 정확도
+ 3-gram(tri-gram)은 세 개의 연속된 단어가 얼마나 문장에서 일치하는 지에 대한 정확도
+ 4-gram(four-gram)은 네 개의 연속된 단어가 얼마나 문장에서 일치하는 지에 대한 정확도
+ 주어장 문장에 비해 예측한 문장의 수가 어떻게 되는지에 따라 Brevity penaly가 주어짐
    + 주어진 문장 10단어, 예측한 문장 9단어 - Brevity penaly = 9/10
+ BLEU는 N-gram개의 기하 평균한 값과 Brevity penalty를 곱한 결과가 BLEU값이 됨

![캡처](https://user-images.githubusercontent.com/44515744/108156540-3365c100-7124-11eb-9c5d-b5f2af0fc391.PNG)