## [Day 24] 정점 표현 & 추천시스템 (심화)
### [Graph 7강] 그래프의 정점을 어떻게 벡터로 표현할까?
#### 1. 정점 표현 학습이란?
+ 정점 표현 학습이란 그래프의 정점들을 벡터의 형태로 표현하는 것
+ 정점 표현 학습은 간단히 정점 임베딩(Node Embedding)이라고도 부름
+ 정점 임베딩은 벡터 형태의 표현 그 자체를 의미하기도 함
+ 정점이 표현되는 벡터 공간을 임베딩 공간이라고 부름
+ 정점 표현 학습의 입력은 그래프이고, 주어진 그래프의 각 정점 u에 대한 임베딩, 즉 벡터 표현 z_{u}가 정점 임베딩의 출력

![캡처](https://user-images.githubusercontent.com/44515744/109088704-46dfe000-7753-11eb-8df9-8868a439e915.PNG)

#### 2. 정점 표현 학습의 이유
+ 정점 임베딩의 결과로, 벡터 형태의 데이터를 위한 도구들을 그래프에도 적용할 수 있음
+ 대부분 분류기(로지스틱 회귀분석, 다측 퍼셉트론 등) 그리고 군집 분석 알고리즘(K-Means, DBSCAN 등)은 벡터 형태로 표현된 사례(Instance)들을 입력으로 받음
+ 그래프의 정점들을 벡터 형태로 표현할 수 있다면, 위의 예시와 같은 대표적인 도구들 뿐 아니라, 최신의 기계학습 도구들을 정점 분류(Node Classification), 군집 분석(Community Detection) 등에 활용할 수 있음

#### 3. 정점 표현 학습의 목표
+ 그래프에서의 정점간 유사도를 임베딩 공간에서도 "보존"하는 것을 목표로 함

![캡처](https://user-images.githubusercontent.com/44515744/109089312-61668900-7754-11eb-928c-c1aa6549d909.PNG)

+ 임베딩 공간에서의 유사도는 내적(Inner Product)를 사용함
+ 임베딩 공간에서의 u와 v의 유사도는 둘의 임베딩의 내적임
    + 내적은 두 벡터가 클 수록, 그리고 같은 방향을 향할 수록 큰 값을 가짐

![캡처](https://user-images.githubusercontent.com/44515744/109089659-f36e9180-7754-11eb-8d05-337217a42d19.PNG)

#### 4. 정점 표현 학습의 목표
+ 정점 임베딩은 다음 두 단계로 이루어짐
    + 그래프에서의 정점 유사도를 정의하는 단계
    + 정의한 유사도를 보존하도록 정점 임베딩을 학습하는 단계

#### 5. 인접성 기반 접근성
+ 인접성(Adjacency) 기반 접근법에서는 두 정점이 인접할 때 유사하다고 간주함
+ 두 정점 u와 v가 인접하다는 것은 둘을 직접 연결하는 간선 (u,v)가 있음을 의미함
+ 인접행렬(Adjacency Matrix) A의 u행 v열 원소 A_{u,v}는 u와 v가 인접한 경우 1 아닌 경우 0
+ 윈접행렬의 원소 A_{u,v}를 두 정점 u와 v의 유사도로 가정함

![캡처](https://user-images.githubusercontent.com/44515744/109090935-2f0a5b00-7757-11eb-9d3c-2e284346fa56.PNG)

+ 인접성 기반 접근법의 손실 함수(Loss Function)는 아래와 같음
+ 손실 함수가 최소가 되는 정점 임베딩을 찾는 것을 목표로함, 손실 함수 최소화를 위해서는 (확률적) 경사하강법 등이 사용됨

![캡처](https://user-images.githubusercontent.com/44515744/109091180-a3dd9500-7757-11eb-96ea-ecd91f3aae49.PNG)

#### 6. 인접성 기반 접근법의 한계
+ 인접성만으로 유사도를 판단하는 것은 한계가 있음
    + 각 정점에 대한 거리가 다른데 인접성으로 0,1로 구분한다면 이러한 특성들을 표현할 수 없음
    + 그래프에서의 거리와 군집성을 무시함

#### 7. 거리 기반 접근법
+ 두 정점 사이의 거리가 충분히 가까운 경우 유사하다고 간주함

![캡처](https://user-images.githubusercontent.com/44515744/109091491-3c741500-7758-11eb-9571-3c14c26bc874.PNG)

#### 8. 경로 기반 접근법
+ 경로 기반 접근법에서는 두 정점 사이의 경로가 많을 수록 유사하다고 간주함
+ 정점 u와 v의 사이의 경로(Path)는 아래 조건을 만족하는 정점들의 순열(Sequence)임
    + u에서 시작해서 v에서 끝나야함
    + 순열에서 연속된 정점은 간선으로 연결되어 있어야함
    + 인접성 기반의 접근법과는 다르게 k(거리) 제곱이 포함됨

![캡처](https://user-images.githubusercontent.com/44515744/109091804-ca500000-7758-11eb-98e0-e26e679d7ab5.PNG)


#### 9. 중첩 기반 접근법
+ 두 정점이 많은 이웃을 공유할수록 유사하다고 간주함
+ 빨간색 정점은 파란색 정점과 두 명의 이웃을 공유하기 때문에 유사도는 2가 됨

![캡처](https://user-images.githubusercontent.com/44515744/109092145-611cbc80-7759-11eb-912a-ed64cb7ca735.PNG)

+ 정점 u의 이웃 집합을 N(u) 그리고 정점 v의 이웃 집합을 N(v)라고 하면, 두 정점의 공통 이웃 수 S_{u,v}는 아래와 같이 정의됨

![캡처](https://user-images.githubusercontent.com/44515744/109092283-96290f00-7759-11eb-867e-900c0c88c2d7.PNG)

+ 중첨 기반 접근법의 손실 함수는 다음과 같음

![캡처](https://user-images.githubusercontent.com/44515744/109092340-b48f0a80-7759-11eb-9e14-bcada502f3ba.PNG)

+ 공통 이웃 수를 대신 자카드 유사도 혹은 Adamic Adar 점수를 사용할 수도 있음
+ 자카드 유사도(Jaccard Similarity)는 공통 이웃의 수 대신 비율을 계산하는 방식
    + 두 정점의 이웃이 완벽하게 동일할때 자카드 유사도가 1이됨, 그 외는 0~1 사이의 값

![캡처](https://user-images.githubusercontent.com/44515744/109092457-e7390300-7759-11eb-94a8-cf90755b8c3d.PNG)

+ Adamic Adar 점수는 공통 이웃에게 가중치를 부여하여 가중합을 계산하는 방식
    + 공통이웃 w의 연결성을 나눈 것을 가중치로 활용함
    + 트와이스와 같은 연결성이 높은 계정을 공통 지인으로 갖고있는 것은 가중치가 높지않음

![캡처](https://user-images.githubusercontent.com/44515744/109092559-1b142880-775a-11eb-8aa2-d786faf1a779.PNG)

#### 10. 임의보행 기반 접근법
+ 임의보행 기반 접근법에서는 한 정점에서 시작하여 임의보행을 할 때 다른 정점에 도달할 확률을 유사도로 가준함
+ 임의보행이라 현재 정점의 이웃 중 하나를 균일한 확률로 선택하는 이동과정을 반복하는 것
+ 임의보행을 사용할 경우, 시작 정점 주변의 지역적 정보와 그래프 전역 정보를 모두 고려한다는 장점이 있음
+ 임의보행 기반 접근법은 세 단계를 거침
    + 1) 각 정점에서 시작하여 임의보행을 반복 수행
    + 2) 각 정점에서 시작한 임의보행 중 도달한 정점들의 리스트를 구성함, 이 때, 정점 u에서 시작한 임의보행 중 도달한 정점들의 리스트를 N_{R}(u)라고함, 한 정점을 여러 번 도달한 경우, 해당 정점은 N_{R}(u)에 여러 번 포함이 가능
    + 3) 다음 손실함수를 최소화하는 임베딩을 학습함
        + 확률 값은 크면 클수록 좋고
        + -log(확률 값)은 작으면 작을수록 좋음

    ![캡처](https://user-images.githubusercontent.com/44515744/109093446-b528a080-775b-11eb-956a-be15698c6f47.PNG)

    + 임베딩으로 부터 도달 확률을 추정하는 방법은 다음과 같이 추정
        + 소프트맥스 공식과 유사함

    ![캡처](https://user-images.githubusercontent.com/44515744/109093659-105a9300-775c-11eb-9898-243acb2564b5.PNG)

+ 추정한 도달 확률을 사용하여 손실함수를 완성하고 이를 최소화하는 임베딩을 학습함

    ![캡처](https://user-images.githubusercontent.com/44515744/109096011-1ce0ea80-7760-11eb-9b7e-d83a328f1db6.PNG)

#### 11. DeepWalk와 Node2Vec
+ 임의보행의 방법에 따라 DeepWalk와 Node2Vec이 구분됨
+ Deepwalk는 앞서 설명한 기본적인 임의보행을 사용함
    + 현재 정점의 이웃 중 하나를 균일한 확률로 선택하여 이동하는 과정을 반복함
+ Node2Vec은 2차 치우친 임의보행(Second-order Biased Random Walk)을 사용함
    + 현재 정점(예시에서 v)과 직전에 머물렀던 정점(예시에서 u)을 모두 고려하여 다음 정점을 선택함
    + 직전 정점의 거리를 기준으로 경우를 구분하여 차등적인 확률을 부여함
    + 거리가 가까워지는 방향, 거리가 유지되는 방향, 거리가 멀어지는 방향 각각에 차등적인 확률을 부여함

    ![캡처](https://user-images.githubusercontent.com/44515744/109096545-130bb700-7761-11eb-84dd-34d6408471f8.PNG)

+ Node2Vec에서는 부여하는 확률에 따라서 다른 종류의 임베딩을 얻음
    + 멀어지는 방향에 높은 확률을 부여한 경우, 정점의 역할(다리 역할, 변두리 정점 등)이 같은 경우 임베딩이 유사함
    + 가까워지는 방향에 높은 확률을 부여한 경우, 같은 군집(Community)에 속한 경우 임베딩이 유사함

#### 12. 손실 함수 근사
+ 임의보행 기법의 손실함수는 계산에 정점의 수의 제곱에 비례하는 시간이 소요됨(중첩된 합 때문)
+ 따라서 정점이 많은 경우 근사식을 사용하게 됨
    + 모든 정점에 대해서 정규화하는 대신 몇 개의 정점을 뽑아서 비교하는 형태
    + 근사식의 핵심아이디어는 몇개의 정점을 뽑아서 비교하는 것, 이 때 뽑힌 정점들을 네거티브 샘플이라고 부름
    + 기존 수식은 분모부분에서 제곱합이 이뤄지므로 이 부분이 간소화되어야함
    + 네거티브 샘플이 많을 수록 학습이 더욱 안정적

![캡처](https://user-images.githubusercontent.com/44515744/109098720-21f46880-7765-11eb-9342-16c1328198bc.PNG)

#### 13. 변환식 정점 표현 학습과 귀납식 정점 표현 학습
+ 지금까지 소개한 정점 임베딩 방법들을 변환식(Transductive) 방법
+ 변환식(Transductive) 방법은 학습 결과로 정점의 임베딩 자체를 얻는다는 특성이 있음
+ 정점을 임베딩으로 변화시키는 함수, 즉 인코더를 얻는 귀납식(Inductive) 방법과 대조됨

#### 14. 변환식 정점 표현 학습의 한계
+ 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음 
+ 모든 정점에 대한 임베딩을 미리 계산하여 저장해두어야 합니다.
+ 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용할 수 없음

#### 15. 정리
+ 정점 표현 학습
    + 그래프의 정점들을 벡터로 표현하는 것
    + 그래프에서 정점 사이의 유사성을 계산하는 방법에 따라 여러 접근법이 구분됨
+ 인접성 기반 접근법
    + 두 정점이 인접한 경우, 두 정점이 직접 간선으로 연결된 경우 유사하다고 판별
+ 거리/경로/중첩 기반 접근법
    + 거리 기반 접근법에서는 두 정점의 거리가 충분히 가까울때 가깝다고 얘기함
    + 경로 기반 접근법에서는 두 정점 사이의 거리가 k이하인 경로가 많을 경우 가깝다고 얘기함
    + 중첩 기반 접근법에서는 두 정점의 공통 이웃이 많으면 많을수록 유사함
+ 임의보행 기반 접근법
    + 각 정점에서 시작하여 임의보행을 할 때 마주칠 확률이 높으면 높을수록 두 정점이 유사하다고 함
    + 임의보행 방법에 따라 DeepWalk와 Node2Vec으로 구분됨
    + DeepWalk는 기본적인 임의 보행을 했고
    + Node2Vec에서는 2차 치우친 임의 보행을 했음, 이때 가중치를 어떻게 설정하느냐에 따라 학습한 임베딩이 입력 그래프의 서로다른 측면을 나타냄

+ 변환식 정점 표현 학습의 한계
    + 위 내용들은 전부 변환식 임베딩 방법에 해당함, 변환식 임베딩은 정점들을 벡터로 표현한 임베딩 자체를 학습함, 정점들을 벡터로 반환하는 인코더, 즉 그 함수를 배우는 귀납식 방법과 대조됨
    + 귀납식 방법과 대비했을때 변환식 방법은 저장공간을 많이 차지하고 입력 그래프가 바뀌었을때 효율적으로 대응하지 못함, 정점이 가진 부가정보를 활용하지 못함

### [Graph 8강] 그래프를 추천시스템에 어떻게 활용할까? (심화)
#### 1. 추천시스템 기본 복습
+ 추천 시스템은 사용자 각각이 구매할 만한 혹은 선호할 만한 상품/영화/영상을 추천함
+ 추천 시스템의 핵심은 사용자별 구매를 예측하거나 선호를 추정하는 것
    + 그래프 관점에서 추천 시스템은 미래의 간선을 예측하는 문제
    + 누락된 간선의 가중치를 추정하는 문제로 해석이 가능
+ 내용 기반 추천시스템
    + 내용 기반 추천은 각 사용자가 구매/만족했던 상품과 유사한 것을 추천하는 방법
        + 동일한 장르의 영화를 추천
        + 동일한 감독의 영화 혹은 동일 배우가 출현한 영화를 추천하는 것 등
    + 대상 사용자가 있고, 그 사용자가 과거에 선호했거나 구매했던 정보를 갖고 속성 정보를 벡터 형태로 표현 (상품 프로필)
    + 여러 상품이 공통적으로 갖는 특성이 무엇인지를 분석한 것이 사용자 프로필
    + 후보 상품군의 각각의 프로필과 사용자 프로필 사이의 유사도를 측정해서 높은 상품을 사용자에게 추천함
    + 장점
        + 다른 사용자의 구매 기록이 필요하지 않음
        + 독특한 취향의 사용자에게도 추천이 가능
        + 새 상품에 대해서도 추천이 가능
        + 추천의 이류를 제공이 가능
    + 단점
        + 상품에 대한 부가 정보가 없는 경우에는 사용이 불가능
        + 구매 기록이 없는 사용자에게는 사용이 불가능
        + 과적합으로 지나치게 협소한 추천을 할 위험이 존재
+ 협업 필터링
    + 유사한 취향의 사용자들이 선호/구매한 상품을 추천하는 방법
        + 추천의 대상 사용자를 x라고 함
        + x와 유사한 취향의 사용자들을 찾음
        + 다음 단계로 유사한 취향의 사용자들이 선호한 상품을 찾음
        + 마지막으로 이 상품들을 x에게 추천
    + 장점
        + 상품에 대한 부가 정보가 없는 경우에도 사용이 가능
    + 단점
        + 충분한 수의 평점 데이터가 누적되어야 효과적
        + 새 상품, 새ㅔ로운 사용자에 대한 추천이 불가능
        + 독특한 취향의 사용자에게 추천이 어려움
    + 훈련 데이터를 이용하여 추정한 점수를 평가 데이터와 비교하여 정확도를 측정함 (평균 제곱 오차, Mean Squared Error를 주로 사용)

#### 2. 넷플릭스 챌린지 소개
+ 사용자별 영화 평점 데이터가 사용
+ 훈련 데이터(Training Data)는 2000~2005년까지 수집한 48만명 사용자의 1만 8천개의 영화에 대한 1억 개의 평점으로 구성, 평가 데이터(Test Data)는 각 사용자의 최신 평점 280만개로 구성 
+ 당시 넷플릭스의 평균 제곱근 오차가 0.9514을 0.8563까지 낮출 경우 100만불의 상금을 받는 조건
    + 추천시스템의 성능이 비약적으로 발전

#### 3. 잠재 인수 모형 개요
+ 잠재 인수 모형(Latent Factor Model)의 핵심은 사용자와 상품을 벡터로 표현하는 것
+ 이 모형의 핵심은 정점 임베딩

![캡처](https://user-images.githubusercontent.com/44515744/109111997-43614e80-777d-11eb-83fb-a5099a24efc7.PNG)

+ 잠재 인수 모형에서는 고정된 인수 대신 효과적인 인수를 학습하는 것을 목표로 함
    + 추천을 가장 정확하게 할 수 있는 차원을 찾아서 임베딩 공간에 영화와 사람들을 배치함
    + 학습한 인수를 잠재 인수(Latent Factor)라고 함

#### 4. 손실 함수
+ 사용자와 상품을 임베딩하는 기준은?
    + 사용자와 상품의 임베딩의 내적(Innter Product)이 평점과 최대한 유사하도록 하는 것
    + 사용자 x의 임베딩 p_{x}, 상품 i의 임베딩을 q_{i}라고 함
    + 사용자 x의 상품 i에 대한 평점을 r_{xi}라고 함
    + 임베딩의 목표는 p_{x}^{T}이 r_{xi}와 유사하도록 하는 것
+ 사용자의 수의 열과 상품 수의 행을 가진 평점 행렬을 R이라고 함
+ 사용자의 임베딩, 즉 벡터를 쌓아서 만든 사용자 행렬을 P라고 함
+ 영화들의 임베딩, 즉 벡터를 쌓아서 만든 상품 행렬을 Q라고 함

![캡처](https://user-images.githubusercontent.com/44515744/109112685-64766f00-777e-11eb-92ab-00e39af3f16c.PNG)

+ 잠재 인수 모형은 다음 손실 함수를 최소화하는 P와 Q를 찾는 것을 목표로 함
    + 하지만, 위 손실 함수를 사용할 경우 과적합(Overfitting)이 발생할 수 있음

    ![캡처](https://user-images.githubusercontent.com/44515744/109112907-bc14da80-777e-11eb-90f9-3423ab5e7d9c.PNG)

+ 과적합을 방지하기 위하여 정규화 항을 손실 함수에 더해줌
    + 모형 복잡도를 최소함, p와 q가 너무 큰 값을 가지지 않는 것을 의미함
    + 임베딩이 크면 훈련 데이터에 있는 잡음도 배우게 됨
    + 정규화는 극단적인, 즉 절댓값이 너무 큰 임베딩을 방지하는 효과가 있음

![캡처](https://user-images.githubusercontent.com/44515744/109113420-963c0580-777f-11eb-800f-6b071da38f1d.PNG)

#### 5. 최적화
+ 손실화함수를 최소화하는 P와 Q를 찾기 위해서는 (확률적) 경사하강법을 사용함
+ 경사하강법은 손실함수를 안정적으로 하지만 느리게 감소시킴
+ 확률적 경사하강법은 손실함수를 불안정하지만 빠르게 감소시킴
+ 실제로는 확률적 경사하강법이 더 많이 사용됨

#### 6. 고급 잠재 인수 모형 - 사용자와 상품의 편향을 고려한 잠재 인수 모형
+ 각 사용자의 편향은 해당 사용자의 평점 평균과 전체 평점 평균의 차
    + 나연 평균 - 4.0 , 다현 평균 - 3.5일 경우
    + 전체 평균이 3.7개의 별인 경우, 나연의 사용자 편향은 4.0 - 3.7 = 0.3개의 별
    + 다현의 사용자 편향은 3.5 - 3.7 = -0.2개의 별

+ 각 상품의 편향은 해당 상품에 대한 평점 평균과 전체 평점 평균의 차
    + 영화 식스센스에 대한 평점의 평균이 4.5개의 별
    + 영화 클레멘타인이 매긴 평점의 평균이 3.0개의 별
    + 전체 평점 평균이 3.7개의 별인 경우, 식스센스의 상품 편향은 4.5 - 3.7 = 0.8개의 별
    + 클레멘타인의 상품 편향은 3.0 - 3.7 = -0.7개의 별

+ 개선된 잠재 인수 모형에서는 평점을 전체 평균, 사용자 편향, 상품 편향, 상호작용으로 분리함

![캡처](https://user-images.githubusercontent.com/44515744/109114339-d2239a80-7780-11eb-97df-551a1ca3d117.PNG)

+ 개선된 잠재 인수 모형의 손실 함수는 아래와 같음
    + 확률적 경사하강법을 통해 손실 함수를 최소화하는 잠재 인수와 편향을 찾아냄

    ![캡처](https://user-images.githubusercontent.com/44515744/109114608-36465e80-7781-11eb-8bd0-3790cc4b95f6.PNG)

#### 7. 시간적 편향을 고려한 잠재 인수 모형
+ 넷플릭스 시스템의 변화로 평균 평점이 크게 상승하는 사건이 있었음
+ 영화의 평점은 출시일 이후 시간이 지남에 따라 상승하는 경향을 갖음
+ 처음 영화를 본 사람들은 기다렸기 때문에 점수가 높음, 그 다음 본 사람들은 영화의 팬이 아니기 때문에 점수가 하락됨
+ 오래된 영화를 굳이 찾아보진 않으므로 추천을 받거나 진짜 좋아할만한 영화라서 보는 영화일 것, 그래서 평점이 높음
+ 개선된 잠재 인수 모형에서는 시각적 편향을 고려하기 때문에, 사용자 편향과 상품 편향을 시간에 따른 함수로 가정함

![캡처](https://user-images.githubusercontent.com/44515744/109115090-e3b97200-7781-11eb-8348-9885d6235bc6.PNG)

#### 8. 앙상블 학습
+ 넷플릭스 우승팀은 BellKor 팀으로 많은 알고리즘들을 앙상블하여 목표에 도달함

![캡처](https://user-images.githubusercontent.com/44515744/109115392-57f41580-7782-11eb-9583-e560923bfa8c.PNG)

#### 9. Cross Validation(교차 검증)
+ 데이터 셋에 평가 데이터를 제외한 4개의 학습데이터(1,2,3,4)가 존재한다면 
    + 이중 1,2,3을 사용해서 학습을 하고 나머지 4로 검증을 함
    + 그 다음 1,2,4를 사용해서 학습을 하고 나머지 3으로 검증을 함
    + 이렇게 교차하면서 모든 데이터를 학습하여 보다 많은 데이터를 기반으로 학습을 할 수 있음