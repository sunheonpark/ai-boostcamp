## [Day 25] GNN 기초 & GNN 심화
### [Graph 9강] 그래프 신경망이란 무엇일까? (기본)
#### 1. 정점 표현 학습 복습
+ 정점 표현 학습이란 그래프의 정점들을 벡터의 형태로 표현하는 것
+ 정점 표현 학습은 간단히 정점 임베딩(Node Embedding)이라고 부름
+ 주어진 그래프의 각 정점 u에 대한 임베딩, 즉 벡터 표현 z_{u}가 정점 임베딩의 출력
+ 그래프에서의 정점간 유사도를 임베딩 공간에서도 "보존"하는 것을 목표로 함
+ 그래프에서 정점의 유사도(내적으로 계산)를 정의하는 방법에 따라 여러 방법이 존재함
    + 인접성
    + 거리
    + 경로
    + 중첩
    + 임의보행 
+ 변환식(Transductive) 방법은 학습의 결과로 정점의 임베딩 자체를 얻는다는 특성
    + 출력으로 임베딩 자체를 얻는 변환식 방법은 여러 한계
    + 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음
    + 모든 정점에 대한 임베딩을 미리 계산하여 저장해두어야함
    + 정점이 속성(Attribute) 정보를 가진 경우, 이를 활용할 수 없음
+ 귀납식(Inductive)는 정점을 임베딩으로 변화시키는 함수, 즉 인코더를 얻는 것
    + 학습이 진행된 이후에 추가된 정점에 대해서도 임베딩을 얻을 수 있음
    + 모든 정점에 대한 임베딩을 미리 계산하여 저장해둘 필요가 없음
    + 정점이 속성(Attibute) 정보를 가진 경우에 이를 활용할 수 있음
        + 인코더를 디자인할 때 속성정보를 활용할 수 있도록 함수의 구조를 설계하기 때문에 가능

#### 2. 그래프 신경망 기본
+ 그래프 신경망은 그래프와 정점의 속성 정보를 입력으로 받음
+ 그래프의 인접 행렬을 A라고 할 경우 인접 행렬 A는 |V| x |V|의 이진행렬
+ 각 정점 u의 속성(Attribute) 벡터를 X_{u}라고 함
+ 정점 속성 벡터 X_{u}는 m차원 벡터이고, m은 속성의 수를 의미함

#### 3. 그래프 신경망 구조
+ 그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻음
+ 대상 정점의 임베딩을 얻기 위해 이웃들 그리고 이웃의 이웃들의 정보를 집계함 (여러 단계 이웃의 정보를 집계)
+ 각 집계 단계를 층(Layer)이라고 부르고, 각 층마다 임베딩을 얻음
+ 각 층에서는 이웃들의 이전 층 임베딩을 집계하여 새로운 임베딩을 얻음
+ 0번 층, 즉 입력 층의 임베딩으로는 정점의 속성 벡터를 사용함

![캡처](https://user-images.githubusercontent.com/44515744/109242055-c6d27c80-781d-11eb-8f60-d09944eacb6a.PNG)

+ 대상 점점마다 집계되는 정보가 상이함
+ 대상 정점 별 집계되는 구조를 계산 그래프(Computation Graph)라고 부름

![캡처](https://user-images.githubusercontent.com/44515744/109242179-f7b2b180-781d-11eb-8e07-1275cf75e259.PNG)

+ 서로 다른 대상 정점간에도 층 별 집계 함수는 공유함
    + 단, 층별로는 다른 집계함수를 사용함
    + 함수는 가변적인 크기도 동일하게 처리할 수 있어야함
    + 집계 함수는 (1) 이웃들 정보의 평균을 계산하고 (2) 신경망에 적용하는 단계를 거침

![캡처](https://user-images.githubusercontent.com/44515744/109242248-1b75f780-781e-11eb-9787-c86bb0ad04c6.PNG)

![캡처](https://user-images.githubusercontent.com/44515744/109242479-8cb5aa80-781e-11eb-94d7-02b285b75676.PNG)

+ 각 정점 별로 마치막 층에서의 임베딩을 얻고 이것을 출력으로 활용함
+ 그래프 신경망의 학습 변수(Trainable Parameter)는 층 별 신경망의 가중치

![캡처](https://user-images.githubusercontent.com/44515744/109243777-de5f3480-7820-11eb-8b9e-f249fee7f97f.PNG)

#### 4. 그래프 신경망의 손실합수
+ 정점간 거리를 "보존"하는 것을 목표로 할 수 있음
+ 인접성을 기반으로 유사도를 정의하면, 손실 함수는 아래와 같음

![캡처](https://user-images.githubusercontent.com/44515744/109246197-22543880-7825-11eb-82c0-a1971dccf304.PNG)

#### 5. 후속 과제(Downstrem Task)의 손실함수를 이용한 종단종(End-to-End) 학습도 가능
+ 그래프 신경망을 이용하여 정점의 임베팅을 얻음
+ 이를 분류기(Classifier)의 입력으로 사용
+ 각 정점의 유형을 분류
+ 분류기의 손실함수, 예를 들어 교차 엔트로피(Cross Entropy)를, 전체 프로세스의 손실함수로 사용하여 종단종(End-to-End) 학습을 할 수 있음

![캡처](https://user-images.githubusercontent.com/44515744/109246894-63991800-7826-11eb-8087-1a63a8ff21ef.PNG)

![캡처](https://user-images.githubusercontent.com/44515744/109246985-8297aa00-7826-11eb-9c05-cd159a823a87.PNG)


#### 6. 그래프 신경망의 학습
+ 그래프 신경망과 변환적 정점 임베딩을 이용한 정점 분류
+ 그래프 신경망의 종단종(End-to-End) 학습을 통한 분류는, 변환적 정점 임베딩 이후에 별도의 분류기를 학습하는 것보다 정확도가 높음
+ 손실함수를 정의하고 난 뒤에는 학습에 사용할 대상 정점을 결정하여 학습 데이터를 구성
+ 마지막으로 오차역전파(Backpropagation)을 통해 손실함수를 최소화 함

#### 7. 그래프 신경망의 활용
+ 학습된 신경망을 적용하여, 학습에 사용되지 않은 정점의 임베딩을 얻을 수 있음
+ 학습 이후에 추가된 정점의 임베딩도 얻을 수 있음
+ 학습된 그래프 신경망을, 새로운 그래프에 적용할 수 있음

#### 8. 그래프 합성곱 신경망
+ 소개한 것 이외에도 다양한 형태의 집계함수를 사용할 수있음
+ 그래프 합성곱 신경망(Graph Convolutional Network, GCN)의 집계 함수

![캡처](https://user-images.githubusercontent.com/44515744/109284664-741bb380-7863-11eb-97a5-5c091d2ff98b.JPG)

#### 9. GraphSAGE
+ 이웃들의 임베딩을 AGG 함수를 이용해 합친 후,
+ 자신의 임베딩과 연결(Concatenation)하는 점이 독특함

![캡처](https://user-images.githubusercontent.com/44515744/109285307-4420e000-7864-11eb-9315-714836e0cf88.JPG)

+ AGG 함수로는 평균, 풀링, LSTM 등이 사용이 될 수 있음

![캡처](https://user-images.githubusercontent.com/44515744/109285430-6c104380-7864-11eb-9669-367007e2cdad.JPG)

#### 10. 합성곱 신경망과의 비교
+ 합성곱 신경망과 그래프 신경망은 모두 이웃의 정보를 집계하는 과정을 반복함
+ 합성곱 신경망은 이웃 픽셀의 정보를 집계하는 과정을 반복함
+ 합성곱 신경망에서는 이웃의 수가 균일하지만 그래프 신경망에서는 아님, 그래프 신경망에서는 정점 별로 집계하는 이웃의 수가 다름
+ 그래프에는 합성곱 신경망이 아닌 그래프 신경망을 적용해야함
    + 합성곱 신경망이 쓰이는 이미지에서는 인접 픽셀이 유용한 정보를 담고 있을 가능성이 높음
    + 하지만 그래프의 인접 행렬에서의 인접 원소는 제한된 정보를 갖고, 행과 열의 순서는 임의로 결정되는 경우가 많음

#### 11. 정리
+ 정점 표현 학습
    + 그래프의 정점들을 벡터로 표현하는 것
    + 그래프에서 정점 사이의 유사성을 계산하는 방법에 따라 여러 접근법이 구분됨
    + 그래프 신경망 등의 귀납식 정점 표현 학습은 임베딩 함수를 출력으로 얻음

+ 그래프 신경망 기본
    + 그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 어음
    + 후속 과제의 손실함수를 사용해 종단종 학습이 가능함
    + 학습된 그래프 신경망을 학습에서 제외된 정점, 새롭게 추가된 정점, 새로운 그래프에 적용 가능

+ 그래프 신경망 변형
+ 합성곱 신경망과의 비교
    + 그래프 형태의 데이터에는 합성곱 신경망이 아닌 그래프 신경망을 사용해야 효과적

### [Graph 10강] 그래프 신경망이란 무엇일까? (심화)
#### 1. 그래프 신경망 복습
+ 귀납식 정점 표현 학습
    + 정점을 임베딩하는 함수, 즉 인코더를 학습하는 귀납식 정점 표현 학습은 여러 장점을 갖음
        + 학습이 진행된 이후에 추가된 정점에 대해서도 임베딩을 얻을 수 있음
        + 모든 정점에 대한 임베딩을 미리 계산하여 저장이 가능
        + 정점이 속성(Attribute) 정보를 가진 경우에 이를 활용이 가능
+ 그래프 신경망의 구조
    + 그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻음
    + 집계 함수의 형태에 따라, 그래프 신경망, 그래프 합성곱 신경망, GraphSAGE 등이 구분됨
        + 이웃들 정보의 평균을 계산
+ 그래프 신경망의 학습
    + 그래프 신경망은 비지도 학습, 지도 학습이 모두 가능
    + 비지도 학습에서는 정점간 거리를 "보존"하는 것을 목표로 함
    + 지도 학습에서는 후속 과제의 손실함수를 이용해 종단종 학습을 함
+ 그래프 신경망의 활용
    + 학습된 신경망을 적용하여, 학습에 사용되지 않은 정점, 학습 이후에 추가된 정점, 심지어 새로운 그래프의 정점의 임베딩을 얻을 수 있음

#### 2. 기본 그래프 신경망의 한계
+ 기본 그래프 신경망에서는이웃들의 정보를 동일한 가중치로 평균을 냄
+ 그래프 합성곱 신경망에서 역시 단순히 연결성을 고려한 가중치로 평균을 냄

#### 3. 그래프 어텐션 신경망
+ 그래프 어텐션 신경망(Graph Attention Network, GAT)에서는 가중치 자체도 학습함
    + 실제 그래프에서는 이웃 별로 미치는 영향이 다를 수 있기 때문
    + 가중치를 학습하기 위해서 셀프-어텐션(Self-Attention)이 사용도미
+ 각 층에서 정점 i로부터 이웃 j로의 가중치 a_{ij}는 세 단계를 통해 계산
    + 해당 층의 정점 i의 임베딩 h_{i}에 신경망 W를 곱해 새로운 임베딩을 얻음
    + 정점 i와 정점 j의 새로운 임베딩을 연결한 후, 어텐션 계수 a를 내적함, 어텐션 계수 a는 모든 정점이 공유하는 학습 변수
    + 그 결과에 소프트맥스(Softmax)를 적용함
    
    ![캡처](https://user-images.githubusercontent.com/44515744/109297602-9c141280-7875-11eb-9d58-3b1370b07a6e.JPG)

+ 여러 개의 어텐션을 동시에 학습한 뒤, 결과를 연결하여 사용이 가능
    + 이를 멀티헤드 어텐션(Multi-head Attenion)이라고 부름
+ 어텐션의 결과 정점 분류의 정확도(Accuracy)가 향상되는 것을 확인할 수 있음

#### 4. 그래프 표현 학습
+ 그래프 표현 학습, 혹은 그래프 임베딩이란 그래프 전체를 벡터의 형태로 표현하는 것
+ 개별 정점을 벡터의 형태로 표현하는 정점 표현 학습과 구분됨
+ 그래프 임베딩은 벡터의 형태로 표현된 그래프 자체를 의미

+ 그래프 임베딩은 그래프 분류 등에 활용
    + 그래프 형태로 표현된 화합물의 분자 구조로부터 특성을 예측하는 것이 한가지 예시

#### 5. 그래프 풀링
+ 정점 임베딩들로부터 그래프 임베딩을 얻는 과정
+ 평균 등 단순한 방법보다 그래프의 구조를 고려한 방법을 사용할 경우, 그래프 분류 등의 후속 과제에서 더 높은 성능을 얻음
+ 미분가능한 풀링(Differentiable Pooling, DiffPool)은 군집 구조를 활용 임베딩을 계층적으로 집계

![캡처](https://user-images.githubusercontent.com/44515744/109298214-979c2980-7876-11eb-9287-4def35de51a8.JPG)

#### 6. 지나친 획일화 문제
+ 지나친 획일화(Over-smoothing) 문제란 그래프 신경망의 층의 수가 증가하면서 정점의 임베딩이 서로 유사해지는 형상을 의미함
    + 지나친 획일화 문제는 작은 세상 효과와 관련이 있음, 적은 수의 층으로도 다수의 정점에 의해 영향을 받게됨
+ 지나친 획일화의 결과로 그래프 신경망의 층의 수를 늘렸을 때, 후속 과제에서의 정확도가 감소하는 현상이 발견 (2,3개일 때 정확도가 가장 높음
+ 잔차항(Residual)을 넣는 것, 즉 이전 층의 임베딩을 한번 더 더해줄 수 있지만 이 역시도 효과가 제한적
+ JK 네트워크(Jumping Knowledge Network)는 마지막 층의 임베딩 뿐 아니라, 모든 층의 임베딩을 함께 사용

![캡처](https://user-images.githubusercontent.com/44515744/109298566-24df7e00-7877-11eb-8054-1062af12d9a3.JPG)

+ APPNP라는 그래프 신경망에서는 0번째 층을 제외하고는 신경망 없이 집계 함수를 단순화함, W 매트릭스를 0번째에만 넣음

![캡처](https://user-images.githubusercontent.com/44515744/109298820-843d8e00-7877-11eb-93a0-581bc5867f62.JPG)

+ APPNP의 경우, 층의 수 증가에 따른 정확도 감소 효과가 없음

#### 7. 그래프 데이터의 증감
+ 다양한 기계학습 문제에서 효과적
+ 그래프에도 누락되거나 부정확한 간선이 있고, 데이터 증강을 통해 보완이 가능
+ 임의 보행을 통해 정점간 유사도를 계산하고, 유사도가 높은 정점 간의 간선을 추가하는 방법이 제안
    + 입력 데이터에서의 정점과 유사도를 계산한 수치를 간선의 형태로 표현
    + 유사도가 높은 것들을 필터링, 이것과 입력 그래프의 간선을 더해줘서 그래프 신경망의 입력으로 사용

#### 8. 그래프 데이터의 증감 효과
+ 그래프 데이터 증강의 결과 정점 분류의 정확도가 개선되는 것을 확인

![캡처](https://user-images.githubusercontent.com/44515744/109299276-32e1ce80-7878-11eb-80d0-b943fa50787c.JPG)

#### 9. 정리
+ 그래프 신경망 복습
+ 그래프 신경망에서의 어텐션
    + 그래프 어텐션 신경망은 이웃 정점들의 임베딩을 평균내는 과중치도 함께 학습
+ 그래프 표현 학습과 그래프 풀링
    + 정점 임베딩으로부터 그래프 풀링을 통해 전체 그래프 임베딩, 즉 전체 그래프의 벡터 표현을 얻음
+ 지나친 획일화 문제
    + 그래프 신경망의 층 수를 증가시킬 때, 정점 임베딩이 서로 유사해지고, 후속 과제의 정확도가 떨어지는 현상
+ 그래프 데이터 증강
    + 그래프에 간선을 추가한 뒤 그래프 신경망을 학습시키는 방법으로 후속 과제의 정확도를 향상 시킴

