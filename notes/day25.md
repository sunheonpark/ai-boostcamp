## [Day 25] GNN 기초 & GNN 심화
### [Graph 9강] 그래프 신경망이란 무엇일까? (기본)
#### 1. 정점 표현 학습 복습
+ 정점 표현 학습이란 그래프의 정점들을 벡터의 형태로 표현하는 것
+ 정점 표현 학습은 간단히 정점 임베딩(Node Embedding)이라고 부름
+ 주어진 그래프의 각 정점 u에 대한 임베딩, 즉 벡터 표현 z_{u}가 정점 임베딩의 출력
+ 그래프에서의 정점간 유사도를 임베딩 공간에서도 "보존"하는 것을 목표로 함
+ 그래프에서 정점의 유사도(내적으로 계산)를 정의하는 방법에 따라 여러 방법이 존재함
    + 인접성
    + 거리
    + 경로
    + 중첩
    + 임의보행 
+ 정점 임베딩 방법들을 변환식(Transductive) 방법
+ 변환식(Transductive) 방법은 학습의 결과로 정점의 임베딩 자체를 얻는다는 특성
    + 출력으로 임베딩 자체를 얻는 변환식 방법은 여러 한계
    + 학습이 진행된 이후에 추가된 정점에 대해서는 임베딩을 얻을 수 없음
    + 모든 정점에 대한 임베딩을 미리 계산하여 저장해두어야함
    + 정점이 속성(Attribute) 정보를 가진 경우, 이를 활용할 수 없음
+ 귀납식(Inductive)는 정점을 임베딩으로 변화시키는 함수, 즉 인코더를 얻는 것
    + 학습이 진행된 이후에 추가된 정점에 대해서도 임베딩을 얻을 수 있음
    + 모든 정점에 대한 임베딩을 미리 계산하여 저장해둘 필요가 없음
    + 정점이 속성(Attibute) 정보를 가진 경우에 이를 활용할 수 있음
        + 인코더를 디자인할 때 속성정보를 활용할 수 있도록 함수의 구조를 설계하기 때문에 가능

#### 2. 그래프 신경망 기본
+ 그래프 신경망은 그래프와 정점의 속성 정보를 입력으로 받음
+ 그래프의 인접 행렬을 A라고 할 경우 인접 행렬 A는 |V| x |V|의 이진행렬
+ 각 정점 u의 속성(Attribute) 벡터를 X_{u}라고 함
+ 정점 속성 벡터 X_{u}는 m차원 벡터이고, m은 속성의 수를 의미함

#### 3. 그래프 신경망 구조
+ 그래프 신경망은 이웃 정점들의 정보를 집계하는 과정을 반복하여 임베딩을 얻음
+ 대상 정점의 임베딩을 얻기 위해 이웃들 그리고 이웃의 이웃들의 정보를 집계함 (여러 단계 이웃의 정보를 집계)
+ 각 집계 단계를 층(Layer)이라고 부르고, 각 층마다 임베딩을 얻음
+ 각 층에서는 이웃들의 이전 층 임베딩을 집계하여 새로운 임베딩을 얻음
+ 0번 층, 즉 입력 층의 임베딩으로는 정점의 속성 벡터를 사용함

![캡처](https://user-images.githubusercontent.com/44515744/109242055-c6d27c80-781d-11eb-8f60-d09944eacb6a.PNG)

+ 대상 점점마다 집계되는 정보가 상이함
+ 대상 정점 별 집계되는 구조를 계산 그래프(Computation Graph)라고 부름

![캡처](https://user-images.githubusercontent.com/44515744/109242179-f7b2b180-781d-11eb-8e07-1275cf75e259.PNG)

+ 서로 다른 대상 정점간에도 층 별 집계 함수는 공유함
    + 단, 층별로는 다른 집계함수를 사용함
    + 함수는 가변적인 크기도 동일하게 처리할 수 있어야함
    + 집계 함수는 (1) 이웃들 정보의 평균을 계산하고 (2) 신경망에 적용하는 단계를 거침

![캡처](https://user-images.githubusercontent.com/44515744/109242248-1b75f780-781e-11eb-9787-c86bb0ad04c6.PNG)

![캡처](https://user-images.githubusercontent.com/44515744/109242479-8cb5aa80-781e-11eb-94d7-02b285b75676.PNG)

+ 각 정점 별로 마치막 층에서의 임베딩을 얻고 이것을 출력으로 활용함
+ 그래프 신경망의 학습 변수(Trainable Parameter)는 층 별 신경망의 가중치

![캡처](https://user-images.githubusercontent.com/44515744/109243777-de5f3480-7820-11eb-8b9e-f249fee7f97f.PNG)

#### 4. 그래프 신경망의 손실합수
+ 정점간 거리를 "보존"하는 것을 목표로 할 수 있음
+ 인접성을 기반으로 유사도를 정의하면, 손실 함수는 아래와 같음

![캡처](https://user-images.githubusercontent.com/44515744/109246197-22543880-7825-11eb-82c0-a1971dccf304.PNG)

#### 5. 후속 과제(Downstrem Task)의 손실함수를 이용한 종단종(End-to-End) 학습도 가능
+ 그래프 신경망을 이용하여 정점의 임베팅을 얻음
+ 이를 분류기(Classifier)의 입력으로 사용
+ 각 정점의 유형을 분류
+ 분류기의 손실함수, 예를 들어 교차 엔트로피(Cross Entropy)를, 전체 프로세스의 손실함수로 사용하여 종단종(End-to-End) 학습을 할 수 있음

![캡처](https://user-images.githubusercontent.com/44515744/109246894-63991800-7826-11eb-8087-1a63a8ff21ef.PNG)

![캡처](https://user-images.githubusercontent.com/44515744/109246985-8297aa00-7826-11eb-9c05-cd159a823a87.PNG)


