## [DAY 7] 경사하강법
> 미분의 개념과 그래디언트 벡터에 대한 내용
### [AI Math 3강] 경사하강법(순한맛)
#### 1. 미분(differentiation)
+ 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구
+ 변화량(h)를 0으로 수렴하게하고 x의 변화 대비 h의 변화를 비교하여 변화율을 계산함
+ 미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 많이 사용
+ 벡터가 입력인 다변수 함수의 경우에는 편미분(partial differentiation)을 사용
+ 미분을 계산하려면 함수의 모양이 매끄러워야한다(연속)
+ 미분은 함수 f의 주어진 점 (x, f(x))에서의 접선의 기울기를 구함
+ ![캡처](https://user-images.githubusercontent.com/44515744/105783942-0f96db80-5fbb-11eb-8b2d-85d5f6a4c62f.JPG)
``` python
# 미분 계산법
import sympy as sym
from sympy.abc import x
sym.diff(sym.poly(x**2 + 2*x + 3), x)
```
#### 2. 경사상승법, 경사하강법
+ 한 점에서 기울기를 알면 어느 방향으로 점을 움직여야 함수 값이 증가하는지/감소하는 지 알 수 있음
    + 증가시키고 싶다면 미분값을 더하고, 감소시키고 싶으면 미분값을 뺀다
    + f`(x) < 0이면 이를 x에 더하면 왼쪽으로 이동함 
    + f`(x) > 0이면 이를 x에 더하면 오른쪽으로 이동함
    + f`(x) < 0이면 이를 x에 빼면 오른쪽으로 이동함 
    + f`(x) > 0이면 이를 x에 빼면 왼쪽으로 이동함
+ 미분값을 더하면 경사상승법(gradient ascent)이라 하며 함수의 극대값의 위치를 구할 때 사용
+ 미분값을 빼면 경사하강법(gradient descent)이라 하며 함수의 극소값의 위치를 구할 때 사용
+ 경사상승/경사하강 방법은 극값에 도달하면 움직임을 멈춘다.

```python
# 경사하강법
# gradient : 미분을 계산하는 함수
# init : 시작점, lr: 학습률, eps: 알고리즘 종료조건( epilon)
# var : 결과값, 위치
var = init
grad = gradient(var)
while(abs(grad) > eps): 
    var = var - lr * grd # 학습률은 속도와, 정확성과 연관되기 때문에 조심해서 다뤄야함
    grad = gradient(var)
```

#### 3. 변수가 벡터인 경우
+ 벡터가 입력인 다변수 함수의 경우 편미분(partial differntiation)을 사용함
+ 특정 방향의 좌표축으로 이동하는 형식으로 미분하는 것을 편미분이라고 함
+ ei는 i번째 값만 1이고 나머지는 0인 단위벡터, 이를 활용하면 i번째 방향에서의 변화율만 계산이 가능
``` python
# 편미분 계산
import sympy as sym
from sympy.abc import x,y
sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x) # x방향에 대한 편미분을 계산(주어진 함수의 변수 개수만큼 계산이 가능)
```
+ 각 변수 별로 편미분을 계산한 그레디언트(gradient) 벡터를 이용하여 경사하강/경사상승법에 사용할 수 있다. (변수가 여러개면 각 변수 별로 미분값을 다 구해서 이를 벡터에 활용)
+ 기호는 nabla : ![image](https://user-images.githubusercontent.com/44515744/105786339-3acff980-5fc0-11eb-9708-d2af5ca65b21.png)
+ 그레디언트 벡터에 -를 붙이게 되면 최소 값을 구할 수 있음, x에 미분 값을 빼는 것과 같은 원리
``` python
# 경사하강법 알고리즘
var = init
grad = gradient(var)
while(norm(grad) > eps): # 벡터는 절대값 대신 노름(norm)을 계산해서 종료조건을 설정함
    var = var - lr * grad  
    grad = gradient(var)

```
---------------------------

### [AI Math 3강] 경사하강법(매운맛)
#### 1. 경사하강법으로 선형회귀 계수 구하기
+ 선형회귀의 목적식은 ||y - XB||2이고 이를 최소화하는 B를 찾아야 하므로 아래와 같은 그레디언트를 구해야함 
![image](https://user-images.githubusercontent.com/44515744/105792088-59d38900-5fca-11eb-9f21-ca9181f0e963.png)
+ 벡터에 대해서 그레디언트 벡터를 계산해준 다음에 학습율에 해당하는 람다 상수를 곱해서 수렴속도를 정해줌, 현재 백터에서 - 람다 * 그레디언트 백터를 빼게되면 다음 벡터를 구할 수 있음


#### 2. 경사하강법 기반 선형회귀 알고리즘
+ 이론적으로 경사하강법은 미분 가능하고 볼록(convenx)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있음
+ 특히 선형회귀의 경우 목적식 ||y - Xb||2은 회귀계수 B에 대해 볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장됨
+ 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지 않음
``` python
# norm : L2-노름을 계산하는 함수
# lr: 학습률, T : 학습횟수
for t in range(T):
    error = y - X @ beta
    grad = - transpose(X) @ error
    beta = beta - lr * grad

X = np.array([[1,1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3

beta_gd = [10.1, 15.1, -6.5]
X_ = np.array([np.append(x,[1]) for x in X])

for t in range(100): # 학습 횟수가 너무 작으면 수렴이 잘 안됨
    error = y - X_ @ beta_gd
    grand = - np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01 * grad # 학습률을 너무 작게 잡으면 늦게 수렴, 크게 잡으면 불완전하게 움직인다.

print(beta_gd)
```

#### 3. 확률적 경사하강법
+ 확률적 경사하강법(stochastic gradient descent)은 모든 데이터를 사용해서 업데이트하는 것이 아니라 데이터 한개 또는 일부를 활용하여 업데이트
    + 일부 활용 : minibatch-sgd
    + 한개만 활용 : sgd
+ 모든 데이터를 계산한 그레디언트 벡터와 유사하지만 같지 않음, 확률적으로 유사할 것이므로 이를 이용함
+ SDG는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는 것이 가능
+ 매번 다른 미니배치를 사용하기 때문에 곡선 모양이 바뀌게 됨.
+ 극소점이 극대점에서 목적식이 확률적으로 바뀌기 때문에 극소점이 아니게 될 수 있음, 원래 그레디언트 벡터가 0이더라도 다른 그레디언트 벡터가 0이 아니므로 극소점에서 탈출하는 것이 가능
    + Non-Covex함수의 극소점을 구하는데 활용이 가능
+ SGD는 볼록이 아닌 목적식에서도 사용이 가능하므로, 경사하강법보다 머신 러닝 학습에 더 효율적
+ 경사하강법보다 빠르게 최소점에 도달하는 것이 가능
+ minibatch 사이즈를 너무 작게 설정하면 경사하강법보다 더 느리게 수렴할 수 있음

#### 4. 확률적 경사하강법의 원리 : 하드웨어
+ 이미지 데이터를 통째로 경사하강법으로 분석하게 되면 메모리가 부족해서 out of memory가 발생
+ 이미지를 미니배치로 쪼개서 하게되면 빠르게 연산하고 하드웨어 한계를 극복할 수 있음
+ GPU에서 행렬 연산과 모델 패러미터를 업데이트하는 동안 CPU는 전처리와 GPU에서 업로드할 데이터를 준비함

---------------------------
### 추가학습
#### 1. Markdown에 수학기호를 삽입하는 방법
+ VS code 라이브러리 설치 : Markdown+Math
+ 수학 기호 정보 : https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:TeX_%EB%AC%B8%EB%B2%95
#### 2. 벡터의 정의
+ 물리학자 관점 : 공간 상의 한 화살표, 길이와 방향으로 정의
+ 컴퓨터 과학자 관점 : 숫자 자료를 배열한 것, 2차원은 숫자가 두 줄로 배열됐음을 의미
+ 수학자 관점 : 무엇이든 다 벡터가 될 수 있다. 벡터 관련 연산이 성립되면 모두다 벡터
+ 꼬리가 원점에 고정되어 있는 화살표를 떠올리자
+ 선형대수학에서 중요한 모든 것들은 두 관점 사이를 오가는 데에서 맴도는 경향이 있음
+ 관념상 벡터는 세로로 적은 뒤 대괄호로 둘러싸서 나타냄
    + 아래는 x = 2, y =3 그리고 x = 2, y = 3, z = 2에 대한 표현
+ 선형대수에서 벡터는 어떤 대상 사이의 상호작용을 해석하는데 활용

$$
\begin{bmatrix}
2 \\
3
\end{bmatrix}

\begin{bmatrix}
2 \\
3 \\
2
\end{bmatrix}
$$

#### 2. 벡터의 계산
+ 벡터 V와 벡터 W를 더한다면 벡터 V에 벡터 W의 꼬리를 연결하고, 벡터 V의 꼬리에서 부터 벡터 W의 머리를 이은 새로를 벡터가 결과가 됨
+ 벡터는 거리와 방향을 가진 움직임을 나타내는 것

$$
\begin{bmatrix}
1 \\
2
\end{bmatrix}
+
\begin{bmatrix}
3 \\
-1
\end{bmatrix}
=
\begin{bmatrix}
1 + 3 \\
2 + -1
\end{bmatrix}
$$

+ 벡터에 곱을한다면 이 수치만큼 벡터의 길이가 변경됨, 음수를 곱하면 반대 방향으로 방향도 변경
+ 이러한 늘이고, 줄이고, 뒤집는 과정을 스케일이라고 함
+ 곱해지는 상수들을 스칼라라고 함
+ 벡터의 상수배/스칼라배란 각 항에 그 스칼라를 곱하는 것을 의미

$$
2 *
\begin{bmatrix}
3 \\
-1
\end{bmatrix}
=
\begin{bmatrix}
2 * 3\\
2 * -1
\end{bmatrix}
$$

#### 3. 선형 결합
+ 서로 다른 벡터를 스케일하고 더하여 새 벡터를 얻는 모든 연산을 선형 결합이라고 함
+ 좌표평면의 벡터 [2, 3]는 2와 3의 각각의 스칼라로 볼 수 있음
+ 이때 오른쪽을 가리키는 벡터를 $\hat{i}$로 표시하고 'i햇'이나 x-단위벡터라 부름
+ 이때 위를 가리키는 벡터를 $\hat{j}$로 표시하고 'j햇'이나 y-단위벡터라 부름
+ 이 개념으로 벡터를 '스케일된 두 벡터의 합'으로 볼 수 있음
    + (2)$\hat{i}$ + (3)$\hat{j}$
+ $\hat{i}$와 $\hat{j}$를 xy 좌표계의 "기저벡터"라고 함
+ 임의의 두 벡터에 각각의 임의의 스칼라를 택해서 각 벡터를 스케일한 뒤 이를 더하면 모든 2차원 벡터를 만들 수 있다.
+ 두 벡터를 스케일하고 더하여 새 벡터를 얻는 모든 연산을 선형결합이라고 부름
+ x,y가 아닌 다른 기저 벡터 v,w를 사용하면 여전히 숫자쌍과 2차원 벡터 사이의 상호작용을 유효하게 보여주지만, 결과 값은 표준 기저 i햇과 j햇을 사용한 것과 다르다. ( 축을 바꾼 경우에 대한 예시 )
    + 서로 다른 좌표계 사이의 정확한 관계를 설명하는 내용을 이후에 배울 예정

$$
    a\vec{v} + b\vec{w}
$$

+ 두 스칼라 중 하나는 고정해놓고 나머지 하나의 값을 자유롭게 넣는다면. 벡터의 머리가 한 직선을 그리게 됨

#### 4. 생성(선형 생성)
+ 두 스칼라를 모두 자유롭게 한다면 모든 2차원 벡터를 만들 수 있음
+ 두 기저 벡터가 불행히도 같은 직성상에 위치한 경우에는 끝점이 원점을 지나는 한 직선으로 제한됨
+ 벡터의 모음을 다룰때는 각자를 공간상 점으로 나타내는 것이 일반적(머리만 점으로 표시하고, 꼬리는 원점에 있다고 간주함)
+ 벡터 각자는 화살표로, 벡터의 집합은 점으로 생각하는 것이 편리함
+ 대부분의 벡터의 쌍은 결국 무한한 2차원 평면 전체를 생성하기에 이름
+ 두 벡터가 일직선 위에 있다면, 그들의 생성은 직선임


#### 5. 3차원에 벡터의 생성
+ 두 벡터 각자를 스케일하여 더했을 때 얻을 수 있는 모든 가능한 벡터의 집합
+ 끝점은 3차원 공간의 원점을 지나는 어떤한 평면을 그려낼 것
+ 두 벡터의 선형 생성 : 끝점이 평면위에 위치하는 모든 가능한 벡터들의 집합
+ 세 벡터의 선형 결합

$$
    a\vec{v} + b\vec{w} + c\vec{u}
$$

+ 벡터들의 생성은 모든 가능한 선형결합의 집함
+ 세 번째 벡터가 다른 두 벡터의 선형생성에 놓여있다면, 생성은 변화하지 않고 똑같은 평면에 갇힘
+ 선형 종속 : 벡터들 중 최소한 하나는 불필요한 경우, 생성에 아무것도 더하지 못하는 경우
    + 이때 벡터 중 하나가 다른 벡터들의 선형결합으로 표현될 수 있음

$$
    \vec{u} = a\vec{v} + b\vec{w}
$$

+ 선형 독립 : 벡터 모두가 각자 생성에 다른 차원을 구성하는 경우

$$
    \vec{u} \ne a\vec{v} + b\vec{w}
$$

+ 벡터 공간의 기저 : 공간 전체를 생성하는 선형 독립인 벡터의 집합

#### 6. 선형 변환(*Linear transformations)
+ 선형변환은 한마디로 함수, 벡터(정의역)를 집어 넣으면, 벡터(치역)이 나온다.
    + 특정 벡터를 다른 벡터로 바꾸는 변환 같은 것
    + 입력벡터를 이동시켜서 출력벡터로 만드는 것으로 이해할 수 있음
+ 변환이 선형적이다는 것은 두 가지 속성을 의미함
    + 모든 선들을 변환 이후에도 직선
    + 원점은 변환 이후에도 여전히 원점이여야 함
+ 선형변환은 격자 라인들이 변형 이후에도 여전히 "평행"하고 "동일한" 간격으로 있어야함
+ 선형의 변환은 두 개의 기저벡터(i-hat, j-hat)이 어떻게 변하는지만 알면 해결
+ 변환전에 v벡터를 이루는 i-hat 과 j-hat의 어떤 선형 결합은 변환 후에도 유지됨
    + i-hat과 j-hat의 변형위치만 알면, 벡터 v를 추론할 수 있음
    + $\vec{v} = -1\hat{i} + 2\hat{j}

$$
\begin{bmatrix}
x \\
y
\end{bmatrix}
->
x\hat{i} + y\hat{j}
=
x
\begin{bmatrix}
1 \\
0
\end{bmatrix}
+
y
\begin{bmatrix}
0 \\
1
\end{bmatrix}
=
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$

$$
\begin{bmatrix}
x \\
y
\end{bmatrix}
->
x
\begin{bmatrix}
1 \\
-2
\end{bmatrix}
+
y
\begin{bmatrix}
3 \\
0
\end{bmatrix}
=
\begin{bmatrix}
1x + 3y \\
-2x + 0y
\end{bmatrix}
$$

+ 변형된 i-hat(3,-2)과 j-hat(2,1)은 2x2 Matrix로 표현한다 

$$
\begin{bmatrix}
1 & 3 \\
-2 & 0
\end{bmatrix}
$$

#### 7. Rotation 변환 
+ 모든 공간을 시계 반대방향 90도로 돌리게 되면 i-hat의 좌표는(0,1). j-hat의 좌표는 (-1,0)이 된다. 다른 벡터들의 변화를 파악하기 위해서는 아래와 같이 곱하기만 하면 된다.

$$
\begin{bmatrix}
0 & -1 \\
1 & 0 
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
=
\begin{bmatrix}
0x-y \\
x+0y
\end{bmatrix}
$$

#### 8. Shear 변환 ( 격자가 기울어진다 - 미는 )
+ i-hat(1,0)은 고정이고 j-hat(1,1)이 움직인 변환

$$
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$

#### 9. 행렬
+ 기저벡터들의 변형후 좌표값
+ 행렬의 열들은 이 좌표값을 나타냄
+ 한 행렬은 하나의 선형변환을 나타냄
+ 행렬 - 벡터 곱셈은 그 벡터를 선형변환하는 것과 같음


#### 10. 두 개의 선형변환의 합성
+ Rotaion 한 후 Shear 변환을 할 수있다.
+ 이 선형변환 또한 변환된 기저벡트의 좌표로 파악할 수 있다.
+ 좌측 Shear 변환, Roatation 변환에 대한 행렬은 우측의 Composition된 행렬을 곱한것과 동일한 결과

$$
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
(
\begin{bmatrix}
0 & -1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
)
=
\begin{bmatrix}
1 & -1 \\
1 & 0
\end{bmatrix}
\begin{bmatrix}
x \\
y
\end{bmatrix}
$$

+ 두 행렬의 곱셈은 기하학적으로 한 변환을 적용하고나서 다른 변환을 적용한 것과 같음
+ 행렬의 변환(곱)은 우측에서부터 읽어야한다. Rotation 변환 -> Shear 변환
      + 이것은 함수를 변수의 왼쪽에다가 적는 함수 표기법에서 유래한 것

$$
\begin{bmatrix}
1 & 1 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
0 & -1 \\
1 & 0 
\end{bmatrix}
=
\begin{bmatrix}
1 & -1 \\
1 & 0
\end{bmatrix}
$$

#### 11. 3차원 벡터
+ x축 단위벡터 $\hat{i}$와 y축 단위벡터 $\hat{j}$그리고 z축 단위벡터 $\hat{z}$로 구성됨

$$
\begin{bmatrix}
2 \\
6 \\
-1
\end{bmatrix}
$$

#### 12. 3차원 벡터의 변환
+ 만약 y축을 기준으로 90도를 회전하면 아래와 같아짐
+ 3차원에서 방향을 설명하는 오른손 규칙에 의하면 검지는 \hat{i}, 중지는 \hat{j}, 엄지는 \hat{k}을 의미함.

$$
\begin{bmatrix}
0 \\
0 \\
-1
\end{bmatrix}

\begin{bmatrix}
0 \\
1 \\
0
\end{bmatrix}

\begin{bmatrix}
1 \\
0 \\
0
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 1\\
0 & 1 & 0\\
-1 & 0 & 0
\end{bmatrix}
$$ 

+ 3차원 벡터의 변환 값은 2차원 벡터에서 했던 방식과 동일
+ 각 기저벡터들을 벡터로 스케일링해서 합치면 원하는 결과벡터를 얻게됨 

$$
\vec{v}=
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
= x\hat{i}+y\hat{j}+z\hat{k}
$$

$$
\begin{bmatrix}
0 & 1 & 2 \\
3 & 4 & 5 \\
6 & 7 & 8
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
= x
\begin{bmatrix}
0 \\
3 \\
6
\end{bmatrix}
+ y
\begin{bmatrix}
1 \\
4 \\
7
\end{bmatrix}
+z
\begin{bmatrix}
2 \\
5 \\
8
\end{bmatrix}
$$ 

#### 13. 행렬식
+ 선형변환은 공간을 확대하고 축소시키는 개념으로도 볼 수 있음
+ 아래 행렬의 경우에는 \hat{i}가 3만큼, \hat{j}가 2만큼 증가하여 이 두 기저벡터를 이루는 사각형의 크기가 기존의 A에서 6A로 6배(2X6) 증가했음을 알 수 있음

$$
\begin{bmatrix}
3 & 0 \\
0 & 2
\end{bmatrix} 
$$

+ 격자선이 평행하고 균등한 거리를 유지하면서 선형변환이 발생하기 때문에 도형도 동일한 비율로 스케일링이 됨
+ 선행변환에 의한 영역의 변화를 나타내는 팩터로 행렬식(determinant)이라고 부름
+ det가 3이면 영역크기가 3으로 커짐, det가 1/2이면 절반으로 축소됨
+ det가 0이 되면 모든 공간이 찌부러져서 선 또는 점이될 수 있음
+ 주어진 행렬의 행렬식이 0인지 아닌지 확인하는 것은, 계산할 수 있는지 없는지를 알려주는 것
+ det는 음수가 될 수 있음, 이 경우는 공간이 뒤집어질 경우에 해당 (종이 뒷면 뒤집듯이)
    + 이때는 변환 후(뒤집어진 상태)의 i-hat이 j-hat의 좌측으로 오게됨
+ \hat{i}가 점점 \hat{j}와 가까워지면서 공간이 찌부러진다. i와 j가 하나의 선을 이루게 되면 행력식은 0이 되는데 \hat{i}가 계속 이동하게 되면 행력식 값이 음수가 되는게 더 자연스럽다.

$$
det(
\begin{bmatrix}
3 & 2 \\
0 & 2
\end{bmatrix}
)
=6
$$

#### 14. 3차원에서의 행렬식
+ 3차원에서는 \hat{i}, \hat{j}, \hat{k}로 구성된 1x1x1 큐브(정육면체)에 집중한다.
+ 변환이 일어나면 큐브는 기울어지고 짜부된 큐브(평행육면체)가 된다.
+ 3차원에서는 평행육면체의 부피값이 행렬식이 된다.
+ 행렬식이 0이면 평면이나,선 또는 점이 됐을 때를 의미한다.
+ 변환 후에도 오른손 규칙이 적용(방향이 바뀌지 않는 경우)되면 양수 행렬식을 가진 경우고, 변환 후에 방향이 바껴 왼손으로 바꿔야하는 경우는 행렬식의 값이 음수가 된 것
+ 2차원 행렬식 공식
    + b, c가 0일 경우에는 Area가 정사각형
    + b, c값 중 하나만 0이면 평행사변형
    + b, c가 둘다 0이 아닐 경우에는 평행사변형이 얼마나 대각선 방향으로 늘려지거나 찌그러지는 지를 알려줌

$$
det(
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
)=ad-bc
$$

+ 3차원 행렬식 공식

$$
det(
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}
)=adet(
\begin{bmatrix}
e & f \\
h & i
\end{bmatrix}    
)-bdet(
\begin{bmatrix}
d & f \\
g & i
\end{bmatrix}
)+cdet(
\begin{bmatrix}
d & e \\
g & h
\end{bmatrix}
)
$$