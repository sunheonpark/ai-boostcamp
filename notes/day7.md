## [DAY 7] 경사하강법
> 미분의 개념과 그래디언트 벡터에 대한 내용
### [AI Math 3강] 경사하강법(순한맛)
#### 1. 미분(differentiation)
+ 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구
+ 변화량(h)를 0으로 수렴하게하고 x의 변화 대비 h의 변화를 비교하여 변화율을 계산함
+ 미분(differentiation)은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 많이 사용
+ 벡터가 입력인 다변수 함수의 경우에는 편미분(partial differentiation)을 사용
+ 미분을 계산하려면 함수의 모양이 매끄러워야한다(연속)
+ 미분은 함수 f의 주어진 점 (x, f(x))에서의 접선의 기울기를 구함
+ ![캡처](https://user-images.githubusercontent.com/44515744/105783942-0f96db80-5fbb-11eb-8b2d-85d5f6a4c62f.JPG)
``` python
# 미분 계산법
import sympy as sym
from sympy.abc import x
sym.diff(sym.poly(x**2 + 2*x + 3), x)
```
#### 2. 경사상승법, 경사하강법
+ 한 점에서 기울기를 알면 어느 방향으로 점을 움직여야 함수 값이 증가하는지/감소하는 지 알 수 있음
    + 증가시키고 싶다면 미분값을 더하고, 감소시키고 싶으면 미분값을 뺀다
    + f`(x) < 0이면 이를 x에 더하면 왼쪽으로 이동함 
    + f`(x) > 0이면 이를 x에 더하면 오른쪽으로 이동함
    + f`(x) < 0이면 이를 x에 빼면 오른쪽으로 이동함 
    + f`(x) > 0이면 이를 x에 빼면 왼쪽으로 이동함
+ 미분값을 더하면 경사상승법(gradient ascent)이라 하며 함수의 극대값의 위치를 구할 때 사용
+ 미분값을 빼면 경사하강법(gradient descent)이라 하며 함수의 극소값의 위치를 구할 때 사용
+ 경사상승/경사하강 방법은 극값에 도달하면 움직임을 멈춘다.

```python
# 경사하강법
# gradient : 미분을 계산하는 함수
# init : 시작점, lr: 학습률, eps: 알고리즘 종료조건( epilon)
# var : 결과값, 위치
var = init
grad = gradient(var)
while(abs(grad) > eps): 
    var = var - lr * grd # 학습률은 속도와, 정확성과 연관되기 때문에 조심해서 다뤄야함
    grad = gradient(var)
```

#### 3. 변수가 벡터인 경우
+ 벡터가 입력인 다변수 함수의 경우 편미분(partial differntiation)을 사용함
+ 특정 방향의 좌표축으로 이동하는 형식으로 미분하는 것을 편미분이라고 함
+ ei는 i번째 값만 1이고 나머지는 0인 단위벡터, 이를 활용하면 i번째 방향에서의 변화율만 계산이 가능
``` python
# 편미분 계산
import sympy as sym
from sympy.abc import x,y
sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x) # x방향에 대한 편미분을 계산(주어진 함수의 변수 개수만큼 계산이 가능)
```
+ 각 변수 별로 편미분을 계산한 그레디언트(gradient) 벡터를 이용하여 경사하강/경사상승법에 사용할 수 있다. (변수가 여러개면 각 변수 별로 미분값을 다 구해서 이를 벡터에 활용)
+ 기호는 nabla : ![image](https://user-images.githubusercontent.com/44515744/105786339-3acff980-5fc0-11eb-9708-d2af5ca65b21.png)
+ 그레디언트 벡터에 -를 붙이게 되면 최소 값을 구할 수 있음, x에 미분 값을 빼는 것과 같은 원리
``` python
# 경사하강법 알고리즘
var = init
grad = gradient(var)
while(norm(grad) > eps): # 벡터는 절대값 대신 노름(norm)을 계산해서 종료조건을 설정함
    var = var - lr * grad  
    grad = gradient(var)

```
---------------------------

### [AI Math 3강] 경사하강법(매운맛)
#### 1. 경사하강법으로 선형회귀 계수 구하기
+ 선형회귀의 목적식은 ||y - XB||2이고 이를 최소화하는 B를 찾아야 하므로 아래와 같은 그레디언트를 구해야함 
![image](https://user-images.githubusercontent.com/44515744/105792088-59d38900-5fca-11eb-9f21-ca9181f0e963.png)
+ 벡터에 대해서 그레디언트 벡터를 계산해준 다음에 학습율에 해당하는 람다 상수를 곱해서 수렴속도를 정해줌, 현재 백터에서 - 람다 * 그레디언트 백터를 빼게되면 다음 벡터를 구할 수 있음


#### 2. 경사하강법 기반 선형회귀 알고리즘
+ 이론적으로 경사하강법은 미분 가능하고 볼록(convenx)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있음
+ 특히 선형회귀의 경우 목적식 ||y - Xb||2은 회귀계수 B에 대해 볼록함수이기 때문에 알고리즘을 충분히 돌리면 수렴이 보장됨
+ 비선형회귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지 않음
``` python
# norm : L2-노름을 계산하는 함수
# lr: 학습률, T : 학습횟수
for t in range(T):
    error = y - X @ beta
    grad = - transpose(X) @ error
    beta = beta - lr * grad

X = np.array([[1,1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3

beta_gd = [10.1, 15.1, -6.5]
X_ = np.array([np.append(x,[1]) for x in X])

for t in range(100): # 학습 횟수가 너무 작으면 수렴이 잘 안됨
    error = y - X_ @ beta_gd
    grand = - np.transpose(X_) @ error
    beta_gd = beta_gd - 0.01 * grad # 학습률을 너무 작게 잡으면 늦게 수렴, 크게 잡으면 불완전하게 움직인다.

print(beta_gd)
```

#### 3. 확률적 경사하강법
+ 확률적 경사하강법(stochastic gradient descent)은 모든 데이터를 사용해서 업데이트하는 것이 아니라 데이터 한개 또는 일부를 활용하여 업데이트
    + 일부 활용 : minibatch-sgd
    + 한개만 활용 : sgd
+ 모든 데이터를 계산한 그레디언트 벡터와 유사하지만 같지 않음, 확률적으로 유사할 것이므로 이를 이용함
+ SDG는 데이터의 일부를 가지고 패러미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는 것이 가능
+ 매번 다른 미니배치를 사용하기 때문에 곡선 모양이 바뀌게 됨.
+ 극소점이 극대점에서 목적식이 확률적으로 바뀌기 때문에 극소점이 아니게 될 수 있음, 원래 그레디언트 벡터가 0이더라도 다른 그레디언트 벡터가 0이 아니므로 극소점에서 탈출하는 것이 가능
    + Non-Covex함수의 극소점을 구하는데 활용이 가능
+ SGD는 볼록이 아닌 목적식에서도 사용이 가능하므로, 경사하강법보다 머신 러닝 학습에 더 효율적
+ 경사하강법보다 빠르게 최소점에 도달하는 것이 가능
+ minibatch 사이즈를 너무 작게 설정하면 경사하강법보다 더 느리게 수렴할 수 있음

#### 4. 확률적 경사하강법의 원리 : 하드웨어
+ 이미지 데이터를 통째로 경사하강법으로 분석하게 되면 메모리가 부족해서 out of memory가 발생
+ 이미지를 미니배치로 쪼개서 하게되면 빠르게 연산하고 하드웨어 한계를 극복할 수 있음
+ GPU에서 행렬 연산과 모델 패러미터를 업데이트하는 동안 CPU는 전처리와 GPU에서 업로드할 데이터를 준비함

---------------------------
### 추가학습
#### 1. Markdown에 수학기호를 삽입하는 방법
+ VS code 라이브러리 설치 : Markdown+Math
+ 수학 기호 정보 : https://ko.wikipedia.org/wiki/%EC%9C%84%ED%82%A4%EB%B0%B1%EA%B3%BC:TeX_%EB%AC%B8%EB%B2%95
#### 2. 벡터의 정의
+ 물리학자 관점 : 공간 상의 한 화살표, 길이와 방향으로 정의
+ 컴퓨터 과학자 관점 : 숫자 자료를 배열한 것, 2차원은 숫자가 두 줄로 배열됐음을 의미
+ 수학자 관점 : 무엇이든 다 벡터가 될 수 있다. 벡터 관련 연산이 성립되면 모두다 벡터
+ 꼬리가 원점에 고정되어 있는 화살표를 떠올리자
+ 선형대수학에서 중요한 모든 것들은 두 관점 사이를 오가는 데에서 맴도는 경향이 있음
+ 관념상 벡터는 세로로 적은 뒤 대괄호로 둘러싸서 나타냄
    + 아래는 x = 2, y =3 그리고 x = 2, y = 3, z = 2에 대한 표현
+ 선형대수에서 벡터는 어떤 대상 사이의 상호작용을 해석하는데 활용

![캡처](https://user-images.githubusercontent.com/44515744/105861909-8de29480-6032-11eb-97e2-3f66e8a6f37e.JPG)


#### 2. 벡터의 계산
+ 벡터 V와 벡터 W를 더한다면 벡터 V에 벡터 W의 꼬리를 연결하고, 벡터 V의 꼬리에서 부터 벡터 W의 머리를 이은 새로를 벡터가 결과가 됨
+ 벡터는 거리와 방향을 가진 움직임을 나타내는 것

![캡처](https://user-images.githubusercontent.com/44515744/105861881-86bb8680-6032-11eb-9dc2-04accfd204e5.JPG)

+ 벡터에 곱을한다면 이 수치만큼 벡터의 길이가 변경됨, 음수를 곱하면 반대 방향으로 방향도 변경
+ 이러한 늘이고, 줄이고, 뒤집는 과정을 스케일이라고 함
+ 곱해지는 상수들을 스칼라라고 함
+ 벡터의 상수배/스칼라배란 각 항에 그 스칼라를 곱하는 것을 의미

![캡처](https://user-images.githubusercontent.com/44515744/105861844-7d321e80-6032-11eb-86ef-607b820e5d39.JPG)

#### 3. 선형 결합
+ 서로 다른 벡터를 스케일하고 더하여 새 벡터를 얻는 모든 연산을 선형 결합이라고 함
+ 좌표평면의 벡터 [2, 3]는 2와 3의 각각의 스칼라로 볼 수 있음
+ 이때 오른쪽을 가리키는 벡터를 i-hat로 표시하고 'i햇'이나 x-단위벡터라 부름
+ 이때 위를 가리키는 벡터를 j-hat로 표시하고 'j햇'이나 y-단위벡터라 부름
+ 이 개념으로 벡터를 '스케일된 두 벡터의 합'으로 볼 수 있음
    + (2)*i-hat + (3)*j-hat
+ i-hat와 j-hat를 xy 좌표계의 "기저벡터"라고 함
+ 임의의 두 벡터에 각각의 임의의 스칼라를 택해서 각 벡터를 스케일한 뒤 이를 더하면 모든 2차원 벡터를 만들 수 있다.
+ 두 벡터를 스케일하고 더하여 새 벡터를 얻는 모든 연산을 선형결합이라고 부름
+ x,y가 아닌 다른 기저 벡터 v,w를 사용하면 여전히 숫자쌍과 2차원 벡터 사이의 상호작용을 유효하게 보여주지만, 결과 값은 표준 기저 i햇과 j햇을 사용한 것과 다르다. ( 축을 바꾼 경우에 대한 예시 )
    + 서로 다른 좌표계 사이의 정확한 관계를 설명하는 내용을 이후에 배울 예정

![캡처](https://user-images.githubusercontent.com/44515744/105861680-5378f780-6032-11eb-8906-8d8723c6757d.JPG)

+ 두 스칼라 중 하나는 고정해놓고 나머지 하나의 값을 자유롭게 넣는다면. 벡터의 머리가 한 직선을 그리게 됨

#### 4. 생성(선형 생성)
+ 두 스칼라를 모두 자유롭게 한다면 모든 2차원 벡터를 만들 수 있음
+ 두 기저 벡터가 불행히도 같은 직성상에 위치한 경우에는 끝점이 원점을 지나는 한 직선으로 제한됨
+ 벡터의 모음을 다룰때는 각자를 공간상 점으로 나타내는 것이 일반적(머리만 점으로 표시하고, 꼬리는 원점에 있다고 간주함)
+ 벡터 각자는 화살표로, 벡터의 집합은 점으로 생각하는 것이 편리함
+ 대부분의 벡터의 쌍은 결국 무한한 2차원 평면 전체를 생성하기에 이름
+ 두 벡터가 일직선 위에 있다면, 그들의 생성은 직선임


#### 5. 3차원에 벡터의 생성
+ 두 벡터 각자를 스케일하여 더했을 때 얻을 수 있는 모든 가능한 벡터의 집합
+ 끝점은 3차원 공간의 원점을 지나는 어떤한 평면을 그려낼 것
+ 두 벡터의 선형 생성 : 끝점이 평면위에 위치하는 모든 가능한 벡터들의 집합
+ 세 벡터의 선형 결합

![캡처](https://user-images.githubusercontent.com/44515744/105861558-30e6de80-6032-11eb-8888-c5d65c280bc4.JPG)

+ 벡터들의 생성은 모든 가능한 선형결합의 집함
+ 세 번째 벡터가 다른 두 벡터의 선형생성에 놓여있다면, 생성은 변화하지 않고 똑같은 평면에 갇힘
+ 선형 종속 : 벡터들 중 최소한 하나는 불필요한 경우, 생성에 아무것도 더하지 못하는 경우
    + 이때 벡터 중 하나가 다른 벡터들의 선형결합으로 표현될 수 있음

![캡처](https://user-images.githubusercontent.com/44515744/105861530-288ea380-6032-11eb-8b36-d0a2fbaef977.JPG)

+ 선형 독립 : 벡터 모두가 각자 생성에 다른 차원을 구성하는 경우

![캡처](https://user-images.githubusercontent.com/44515744/105861487-1f053b80-6032-11eb-852f-b513e6471b0d.JPG)

+ 벡터 공간의 기저 : 공간 전체를 생성하는 선형 독립인 벡터의 집합

#### 6. 선형 변환(*Linear transformations)
+ 선형변환은 한마디로 함수, 벡터(정의역)를 집어 넣으면, 벡터(치역)이 나온다.
    + 특정 벡터를 다른 벡터로 바꾸는 변환 같은 것
    + 입력벡터를 이동시켜서 출력벡터로 만드는 것으로 이해할 수 있음
+ 변환이 선형적이다는 것은 두 가지 속성을 의미함
    + 모든 선들을 변환 이후에도 직선
    + 원점은 변환 이후에도 여전히 원점이여야 함
+ 선형변환은 격자 라인들이 변형 이후에도 여전히 "평행"하고 "동일한" 간격으로 있어야함
+ 선형의 변환은 두 개의 기저벡터(i-hat, j-hat)이 어떻게 변하는지만 알면 해결
+ 변환전에 v벡터를 이루는 i-hat 과 j-hat의 어떤 선형 결합은 변환 후에도 유지됨
    + i-hat과 j-hat의 변형위치만 알면, 벡터 v를 추론할 수 있음
    + 벡터 v = -1 x i-hat + 2 x j-hat

![캡처](https://user-images.githubusercontent.com/44515744/105861366-fb41f580-6031-11eb-83da-3eddc331d9d6.JPG)

+ 변형된 i-hat(3,-2)과 j-hat(2,1)은 2x2 Matrix로 표현한다 

![캡처](https://user-images.githubusercontent.com/44515744/105861321-efeeca00-6031-11eb-8546-9723f79baf6d.JPG)


#### 7. Rotation 변환 
+ 모든 공간을 시계 반대방향 90도로 돌리게 되면 i-hat의 좌표는(0,1). j-hat의 좌표는 (-1,0)이 된다. 다른 벡터들의 변화를 파악하기 위해서는 아래와 같이 곱하기만 하면 된다.

![캡처](https://user-images.githubusercontent.com/44515744/105861287-e5cccb80-6031-11eb-9f6c-55874146df3b.JPG)

#### 8. Shear 변환 ( 격자가 기울어진다 - 미는 )
+ i-hat(1,0)은 고정이고 j-hat(1,1)이 움직인 변환

![캡처](https://user-images.githubusercontent.com/44515744/105861242-d9e10980-6031-11eb-83d9-68e95767c14a.JPG)

#### 9. 행렬
+ 기저벡터들의 변형후 좌표값
+ 행렬의 열들은 이 좌표값을 나타냄
+ 한 행렬은 하나의 선형변환을 나타냄
+ 행렬 - 벡터 곱셈은 그 벡터를 선형변환하는 것과 같음


#### 10. 두 개의 선형변환의 합성
+ Rotaion 한 후 Shear 변환을 할 수있다.
+ 이 선형변환 또한 변환된 기저벡트의 좌표로 파악할 수 있다.
+ 좌측 Shear 변환, Roatation 변환에 대한 행렬은 우측의 Composition된 행렬을 곱한것과 동일한 결과

![캡처](https://user-images.githubusercontent.com/44515744/105861192-cb92ed80-6031-11eb-89a4-09b61b65d63b.JPG)

+ 두 행렬의 곱셈은 기하학적으로 한 변환을 적용하고나서 다른 변환을 적용한 것과 같음
+ 행렬의 변환(곱)은 우측에서부터 읽어야한다. Rotation 변환 -> Shear 변환
      + 이것은 함수를 변수의 왼쪽에다가 적는 함수 표기법에서 유래한 것

![캡처](https://user-images.githubusercontent.com/44515744/105861138-bfa72b80-6031-11eb-94c1-b681f3f93bb0.JPG)

#### 11. 3차원 벡터
+ x축 단위벡터 i-hat와 y축 단위벡터 j-hat그리고 z축 단위벡터 z-hat으로 구성됨

![캡처](https://user-images.githubusercontent.com/44515744/105861033-9e463f80-6031-11eb-81fc-94aae0426c4e.JPG)

#### 12. 3차원 벡터의 변환
+ 만약 y축을 기준으로 90도를 회전하면 아래와 같아짐
+ 3차원에서 방향을 설명하는 오른손 규칙에 의하면 검지는 i-hat, 중지는 j-hat, 엄지는 k-hat을 의미함.

![캡처](https://user-images.githubusercontent.com/44515744/105860909-76ef7280-6031-11eb-985d-96171e66f657.JPG)

+ 3차원 벡터의 변환 값은 2차원 벡터에서 했던 방식과 동일
+ 각 기저벡터들을 벡터로 스케일링해서 합치면 원하는 결과벡터를 얻게됨 

![캡처](https://user-images.githubusercontent.com/44515744/105860867-65a66600-6031-11eb-8665-196ff0a7a353.JPG)

#### 13. 행렬식
+ 선형변환은 공간을 확대하고 축소시키는 개념으로도 볼 수 있음
+ 아래 행렬의 경우에는 i-hat이 3만큼, j-hat이 2만큼 증가하여 이 두 기저벡터를 이루는 사각형의 크기가 기존의 A에서 6A로 6배(2X6) 증가했음을 알 수 있음

![캡처](https://user-images.githubusercontent.com/44515744/105860835-5b846780-6031-11eb-93a9-a5b56f54f9a2.JPG)

+ 격자선이 평행하고 균등한 거리를 유지하면서 선형변환이 발생하기 때문에 도형도 동일한 비율로 스케일링이 됨
+ 선행변환에 의한 영역의 변화를 나타내는 팩터로 행렬식(determinant)이라고 부름
+ det가 3이면 영역크기가 3으로 커짐, det가 1/2이면 절반으로 축소됨
+ det가 0이 되면 모든 공간이 찌부러져서 선 또는 점이될 수 있음
+ 주어진 행렬의 행렬식이 0인지 아닌지 확인하는 것은, 계산할 수 있는지 없는지를 알려주는 것
+ det는 음수가 될 수 있음, 이 경우는 공간이 뒤집어질 경우에 해당 (종이 뒷면 뒤집듯이)
    + 이때는 변환 후(뒤집어진 상태)의 i-hat이 j-hat의 좌측으로 오게됨
+ i-hat 점점 j-hat와 가까워지면서 공간이 찌부러진다. i와 j가 하나의 선을 이루게 되면 행력식은 0이 되는데 i-hat이 계속 이동하게 되면 행력식 값이 음수가 되는게 더 자연스럽다.

![캡처](https://user-images.githubusercontent.com/44515744/105860726-3c85d580-6031-11eb-8628-e5115d8e05da.JPG)
#### 14. 3차원에서의 행렬식
+ 3차원에서는 i-hat, j-hat, k-hat로 구성된 1x1x1 큐브(정육면체)에 집중한다.
+ 변환이 일어나면 큐브는 기울어지고 짜부된 큐브(평행육면체)가 된다.
+ 3차원에서는 평행육면체의 부피값이 행렬식이 된다.
+ 행렬식이 0이면 평면이나,선 또는 점이 됐을 때를 의미한다.
+ 변환 후에도 오른손 규칙이 적용(방향이 바뀌지 않는 경우)되면 양수 행렬식을 가진 경우고, 변환 후에 방향이 바껴 왼손으로 바꿔야하는 경우는 행렬식의 값이 음수가 된 것
+ 2차원 행렬식 공식
    + b, c가 0일 경우에는 Area가 정사각형
    + b, c값 중 하나만 0이면 평행사변형
    + b, c가 둘다 0이 아닐 경우에는 평행사변형이 얼마나 대각선 방향으로 늘려지거나 찌그러지는 지를 알려줌
    
![캡처](https://user-images.githubusercontent.com/44515744/105860567-106a5480-6031-11eb-8b5c-057d9a9bccc8.JPG)

+ 3차원 행렬식 공식

![캡처](https://user-images.githubusercontent.com/44515744/105860515-05172900-6031-11eb-807f-e5f5263e1bbb.JPG)

#### 15. 역행렬, 열 공간 및 널 공간
+ 선형대수가 훨씬 광범위하게 적용가능하다고 하는 주된 이유는 어떤 방정식계이든지 해결할 수 있기 때문
+ 방정식계(System of equations)란 미지수인 변수 리스트와 변수들과 관련된 방정식의 리스트를 가진 것을 말함
+ 선형 방정식계 : 각 변수는 상수를 스케일링하는 역할만 하고 스케일된 변수들을 합하는 것이 전부
    + 모든 변수들을 좌항으로 놓고, 상수는 우측으로 옮김
    + 이는 행력-벡터 곱셈이랑 비슷한 형태 => 모든 방정식들을 하나의 벡터방정식으로 표현할 수 있음
    + 공간을 변형시키는 것으로 생각하고 어떤 벡터가 변환되면 벡터 v가 되는지를 찾으면 됨

![캡처](https://user-images.githubusercontent.com/44515744/105867089-0435c580-6038-11eb-9974-b4df31817749.JPG)

+ 방정식을 푸는 방법
    + 행렬 A 변환이 모든 공간을 더 낮은 차원으로 축소시키는지(선이나 점같은 공간) 아니면 공간 전부가 그대로 남는지를 알아보는게 시작점
    + 이는 행렬의 행렬식이 0인지 아니면 0이 아닌지로 나누는 것과 같음
     
+ 역행렬의 정의
    + 역행렬은 A^-1로 표현
    + 역으로 변환하는 것을 역행렬이라고 함, 역행렬을 곱하면 기저벡터를 초기 위치로 되돌림
    + ![캡처](https://user-images.githubusercontent.com/44515744/105869540-ae165180-603a-11eb-9bf4-817933bd6ae7.JPG)은 우측에서 부터 읽으므로 행령A 변환을 역으로 변환하여 결국에는 아무것도 바꾸지 않은 행렬과 같다는 것을 의미한다.
    + 아무것도하지 않는 변환을 "항등 변환(identity transformation)"이라고 한다.
        + i-hat과 j-hat을 이동시키지 않고 나둔다.

![캡처](https://user-images.githubusercontent.com/44515744/105869829-01889f80-603b-11eb-95bd-f4ec4ed64d23.JPG)

+ 역행렬의 활용
    + 만약 A의 역행렬을 구했다면 벡터 V을 역변환하여 벡터 X를 역으로 알 수 있음
        + 이 방적식을 푼다는 것은 역행렬을 v 벡터에 곱한다는 의미
    + 행렬식 값이 0이 아닌 경우에 아무렇게나 행렬을 만들면 대부분 이 경우가 됨
    + 변환 A가 공간을 더 낮은 차원으로 뭉게지 않는한, 즉 행렬식이 0이 아니면 역행렬이 존재함
    + 반대로 변환 A가 더 작은 차원으로 뭉게버린다면 행렬식은 0이 되고 역행렬은 존재하지 않게됨
        + 이때는 벡터 v가 뭉게진 선위에 있을 때만 해를 구할 수 있다.

![캡처](https://user-images.githubusercontent.com/44515744/105870612-cb97eb00-603b-11eb-818d-d291ce1703cf.JPG)

+ 변환의 결과에 따른 명명
    + RANK : 열공간의 차원 수
    + RANK1 : 변환의 결과가 선인 경우, 1차원
    + RANK2 : 모든 벡터가 2차원 평면에 놓여있을 경우
        + 3 x 3 행렬에서, 랭크가 2라는 말은 공간이 붕괴했을음 의미함
    + RANK3 : 3차원 변환의 행렬식 값이 0이 아니라면, 3차원 공간의 결과가 온전한 3차원인 경우
    + 행렬의 점, 선, 평면 등 나올 수 있는 변환의 결과 집합을 "열 공간(column space)라고 함.
        + 행렬의 열들은 기저벡터의 변환 후 위치
        + 열 공간은 이 열들의 확장 공간
        + 영 벡터는 어느 열공간에든지 포함됨 (선형변환은 반드시 원점이 고정되어야 하므로)
    + 차원이 축소될 때 원점으로 이동되는 벡터들의 집합을 행렬의 "영공간(null space)" 혹은 "커널(Kernal)"이라고 부름
    + 방정식에서 벡터 v가 영벡터일 경우, 영공간 모두가 해가될 수 있음
    



